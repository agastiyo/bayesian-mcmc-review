\documentclass[preprint2,longauthor]{aastex631}
\usepackage{blindtext}
\usepackage{amsmath}

% Note to self: Don't forget to start up Zotero when you start writing!

\begin{document}
\label{placeholder} % Delete this when done!

\title{Bayesian Inference and MCMC Methods in Astrophysics}
\author{Agastya Gaur}
\affiliation{University of Illinois at Urbana-Champaign}

\begin{abstract}
  \blindtext
\end{abstract}

\keywords{Astrophysics, Astrostatistics, Bayesian Statistics, Markov Chain Monte Carlo, Big Data}

\section{Introduction}
\label{sec:Introduction}
\subsection{The History of Astrostatistics}
In the 4th century BC, Hipparchus, attempting to estimate the length of a year, found the middle of the range of a scattered set of Babylonian solstice measurements. An achievement for the time, Hipparchus's measurement marked the beginning of what would become a long-standing marriage between astronomy and statistics. In the centuries to come, a number of breakthroughs in astrostatistics followed. Notably, Tycho Brahe in the late 1500s employed repeated positional measurements of stars and used the mean of the data to map them. His work was so precise that it became the foundation of Kepler's laws of planetary motion, and it took astronomers generations to produce better measurements. \citep{leavesleyTychoBrahesWay2018}. Furthermore, in the 1770s, Laplace rediscovered Bayesian statistics, and over the next decade he continued to expand upon his work, using it in a colossal effort to extend Newton's theory of gravity, work that would have him hailed as a monumental genius \citep{stiglerStudiesHistoryProbability1975}.

The biggest advancement in astrostatistics before the era of computing came in 1805 when Legendre published the method of least squares regression to model the orbit of comets \citep{feigelsonStatisticalChallengesModern2004}. He theorized that the model best fit to a set of data was one that minimized the sum of the squares of the errors. Though Legendre did not provide a formal proof of the method, regarding it only as a convenient trick, later works by Robert Adrain developed formal mathematical proofs of the method \citep{merrimanHistoryMethodLeast1877}. In 1809, Gauss published his own work on least squares, showing it was used to calculate the orbit of the dwarf planet Ceres, even when observing it was impossible due to solar glare. Less elegantly, he also insisted that he had discovered the method years before Legendre \citep{stiglerGaussInventionLeast1981}. As controversial as the development of least squares regression ended up being, it has cemented itself in history as one of the most important leaps in astrostatistics.

The recurring theme was clear: progress in astronomy often hinged on solving problems of statistical estimation. By the end of the century, astronomy had firmly established itself as a quantitative science, driven by the refinement of statistical methods to identify regularities in scattered measurements, fitting orbital models, and quantifying uncertainty in the presence of noise.

\subsection{Resolving the Identity Crisis}

The next 100 years brought two developments that reshaped the relationship between astronomy and statistics: the rise of physics as the foundation of astronomy and the advent of computing, which enabled unprecedented scales of data analysis. As astronomy grew increasingly intertwined with the theories of physics, the field transformed into what we now call astrophysics. As more astronomers began to call themselves astrophysicists, statistics began to fade in prominence. Though a niche field called statistical astronomy persisted, the majority of astronomers made little use of statistics in their work \citep{feigelsonStatisticalChallengesModern2004}. The focus shifted to deriving physical models from first principles, and statistical methods were often seen as secondary or even unnecessary. \citet{hubbleDistributionLuminosityElliptical1930} determined the fit for the light curve of elliptical galaxies by trial-and-error instead of regression. \citet{zwickyMassesNebulaeClusters1937} first observed dark matter using a curve fitted only by eye.

The disdain for statistics stemmed from most astrophysicists' stubborn adherence to Newtonian determinism. Physics was regarded as the fundamental law of nature and an elegant basis for astronomy, while statistics was seen as rough, approximate, and imperfect. Around the same time, statistics flourished in the social sciences, further pushing it from astrophysics.

However, statistics would not be kept away from astronomy for long. As computers developed, astronomers increasingly adopted new tools for both calculation and simulation. In the 1920s, one of the earliest applications appeared in the production of lunar tables. Previously, astronomers calculated the position of the Moon using complex, error-prone methods that required extensive manual computation \citep{duncombeEarlyApplicationsComputer1988}. By the 1930s, however, \citet{comrieApplicationHollerithTabulating1932} demonstrated how punch card computing machines could automate the process, making lunar ephemerides faster and more reliable. From then on to the 1970s, Comrie's work was continued by Wallace Eckert, who improved the punch-card calculations using IBM computers \citep{olleyTaskThatExceeded2018}. Like lunar calculations, galactic simulations were also performed with computers as early as the 1940s. \citet{holmbergClusteringTendenciesNebulae1940} modeled gravitational interactions using lightbulbs to represent galaxies, demonstrating how spiral structures could emerge. These analog demonstrations laid the groundwork for fully computational N-body simulations in the 1970s, such as \citet{toomreGalacticBridgesTails1972}, who used them to explain tidal tails and bridges in interacting galaxies.

With these advances in computing came a natural resurgence of statistical methods in astronomy. The ability to automate calculations, handle larger datasets, and simulate complex systems meant that statistical analysis became not only feasible but indispensable. In the mid-20th century, the growth of galaxy surveys encouraged quantitative treatments of structure and dynamics. Early work such as \citet{lynden-bellStatisticalMechanicsViolent1967} applied statistical mechanics to stellar systems, laying the foundation for the study of galaxy formation and equilibrium. By the 1970s, statistics was increasingly recognized as a distinct methodological pillar of astronomy. \citet{peeblesStatisticalAnalysisCatalogs1973} systematically catlogued and analyzed extragalactic objects using power spectra and correlation functions, pioneering the statistical study of large-scale structure.

Where once statistical methods were painfully labor-intensive, computing enabled astronomers to apply them across vast amounts of observational data. While the early history of the field was dominated by statistical reasoning, the growth of physics and computation broadened this into what we now call quantitative analysis (QA). Quantitative analysis thus represents the merging of three traditions that once stood apart: the deductive rigor of physics, the inferential power of statistics, and the scalability of computation. In modern astronomy, progress often relies not on one of these strands in isolation but on their integration. QA therefore serves as both a methodological framework and a philosophy of practice, emphasizing reproducibility, uncertainty quantification, and the ability to extract physical meaning from complex data.

\subsection{The Data Deluge}

Today, astrophysics sits in the middle of a universe of complex statistical problems that demand new quantitative approaches and more computing power with each passing day. In many respects, QA has become the backbone of research in modern astrophysics, and at a pivotal moment as the 21st century has ushered in an unprecedented era of astronomical data generation. Sky surveys like Gaia DR3 alone provide astrometry and photometry for nearly two billion stars, plus more than ten million variable sources across dozens of types \citep{gaiacollaborationGaiaDataRelease2023}. The nineteenth data release of the Sloan Digital Sky Survey collected robust spectroscopic data from over 6 million objects \citep{collaborationNineteenthDataRelease2025}. Advances in CCD detectors will see data from sky surveys continue to grow in the next decade from gigabytes to terabytes today, and possibly to petabytes in the near future. The same trend can be seen in data from NASA's Solar Dynamics Observatory, which now generates over a terabyte of data per day, and the Rubin LSST, generating close to 30 terabytes per day \citep{borneAstroinformatics21stCentury2009}. Compared to the Henry Draper Catalogue \citep{cannonHenryDraperCatalogue1918}—a century-old counterpart that cataloged roughly 200,000 stars—the explosion in data is striking.

The leap from hundreds of thousands of stars in the Henry Draper Catalogue to billions in Gaia represents more than a change in scale: it is a qualitative transformation in what science becomes possible. With the Draper Catalogue, astronomers could classify stellar spectra and trace broad patterns in stellar populations. With Gaia, it is now possible to reconstruct the full three-dimensional structure and kinematics of the Milky Way, identify hypervelocity stars, and test theories of Galactic evolution with unprecedented detail. Where older surveys allowed the identification of a few rare stellar types, modern surveys allow systematic searches for extreme outliers across billions of objects, transforming the statistical character of astronomy.

This data deluge makes QA indispensable. Large-scale surveys now span the entire electromagnetic spectrum, from radio (e.g., LOFAR, ALMA) to X-ray and gamma-ray observatories such as Chandra and Fermi. Multi-messenger astronomy adds yet another layer, with gravitational waves detected by LIGO/Virgo and high-energy neutrinos from IceCube \citep{abbasiSearchIceCubeSubTeV2023}. Time-domain surveys such as ZTF and the upcoming LSST produce streams of transient and variable sources, producing data that are not only high-volume but also high-velocity. This also creates qualitatively harder problems as each modality comes with distinct noise properties, resolutions, and systematic biases, making integration across datasets a formidable statistical task. The ability to extract meaningful insights from these massive datasets in an organized manner is crucial for advancing our understanding of the universe. QA provides a number of powerful tools spanning statistical inference, computational algorithms, and machine learning methodologies to analyze, interpret, and model this data effectively.

\subsection{Statistical Challenges in Modern Astrophysics}

Across astrophysics, two recurring types of challenges emerge. The first challenge is that noisy, incomplete, and often degenerate data have estimated distributions that differ across multiple competing theories. Though theoretical astrophysics has given us the tools to reduce these problems to estimations of physical constants, the number and complexity of parameters still pose major challenges \citep{schaferFrameworkStatisticalInference2015}. Such degeneracies manifest across nearly every subfield of astrophysics. For example, in exoplanet studies, radial velocity measurements can only determine a planet’s minimum mass because they can’t reveal whether we are seeing the orbit at an angle. In cosmology, measurements of the cosmic microwave background blur together the effects of the Hubble constant, the amount of ordinary matter, and dark energy, so isolating any one factor requires assumptions about the others. Even within galaxies, rotation curve studies must weigh the balance between visible stars and invisible dark matter. Each of these cases requires careful statistical treatment to avoid misleading conclusions.

The second challenge is that the large volume of data creates equally daunting problems of computing time and power. The Rubin Observatory LSST will generate tens of terabytes of imaging data per night \citep{borneAstroinformatics21stCentury2009}, while Gaia has already released petabyte-scale catalogs \citep{gaiacollaborationGaiaDataRelease2023}. Brute-force exploration of parameter spaces is simply impossible at these scales. Efficient algorithms and scalable statistical methods are required to render analysis computationally tractable \citep{huijseComputationalIntelligenceChallenges2014}. Without such methods, the majority of information encoded in these massive datasets would remain inaccessible. Together, these issues create a need for QA frameworks that can both handle degeneracy in complex parameter spaces and scale efficiently with massive datasets.

\subsection{Bayesian Inference and MCMC in Context}

Within this landscape, Bayesian inference via Markov Chain Monte Carlo (MCMC) methods naturally emerges as a potential solution.. Bayesian inference offers a principled framework for parameter estimation in complex systems, and MCMC methods provide an effective way to explore parameter spaces by sampling from posterior distributions. For astrophysicists, this has become one of the most widely used and versatile approaches. \citet{vontoussaintBayesianInferencePhysics2011} notes the growing applicability of Bayesian inference in physics. Computational models are becoming far more complex, and the data being analyzed is often noisy and incomplete. Bayesian methods, with their ability to incorporate prior knowledge and handle uncertainty, are particularly well-suited to these challenges. MCMC methods, in particular, provide a practical way to sample from complex posterior distributions that arise in Bayesian analysis. This makes them invaluable for parameter estimation, model comparison, and uncertainty quantification in almost any astrophysical problem.

Another reason for the appeal of Bayesian methods is their contrast with frequentist approaches. Frequentist methods, which were dominant through much of the 20th century, emphasize point estimates and confidence intervals derived from repeated sampling arguments. Bayesian inference, in contrast, provides full posterior probability distributions for parameters, naturally incorporating prior information from physics or earlier observations. This framework is particularly powerful in astrophysics, where data are sparse, noisy, and often incomplete.

The systematic adoption of Bayesian statistics in astronomy gained momentum in the late 20th century. Cosmologists applied Bayesian methods to the cosmic microwave background; for example, works by \citet{tegmarkKarhunenLoeveEigenvalueProblems1997} and others showing how to extract cosmological parameters from noisy sky maps. In exoplanet science, Bayesian inference became standard in the 1990s and 2000s for modeling radial velocity curves and transit signals, which proved successful as datasets grew larger and more precise \citep{gregoryBayesianAnalysisExtrasolar2005}. Pulsar timing, supernova cosmology, and gravitational lens modeling similarly saw Bayesian methods supplanted heuristic or purely frequentist treatments. The trend reflects a broader recognition that many of astronomy’s hardest problems demand not just point estimates, but principled uncertainty quantification.

This review adopts a QA perspective and centers on Bayesian inference and MCMC because of their flexibility, principled uncertainty quantification, and growing ubiquity across astrophysics. In \hyperref[sec:Methodology]{Section II}, the methodology section develops a working foundation: it reviews Bayesian statistics, motivates priors and likelihood construction in realistic astronomical settings, and walks through toy examples before escalating to domain-relevant formulations; it then introduces Monte Carlo methods and builds to Markov Chain Monte Carlo, deriving the Metropolis family and related samplers, and provides step-by-step Python implementations on simple problems.

\hyperref[placeholder]{Section III} presents three focused case studies that illustrate how Bayesian–MCMC pipelines advance frontiers in different subfields while confronting distinct sources of noise, degeneracy, and computational load. For exoplanet direct detection, the section sketches the observational context and the central challenge of disentangling planetary signals from stellar activity and disk structure; it then outlines joint stellar–planet modeling with Gaussian processes and Bayesian classification frameworks for robust detections and non-detections, highlighting benefits and failure modes in low SNR regimes. For CMB parameter estimation, it explains geometric degeneracies in the power spectrum and shows how MCMC accelerators and parallelizable frameworks enable efficient exploration and evidence calculations, including tradeoffs between sampler sophistication and wall-clock efficiency. For gravitational-wave inference, it frames waveform fitting in high-dimensional parameter spaces, discusses the cost of likelihood evaluations, and motivates gradient-informed MCMC strategies that reduce computation without compromising accuracy, with notes on priors, calibration systematics, and real-time constraints.

Finally, \hyperref[placeholder]{Section IV} and \hyperref[placeholder]{V} compare cases where Bayesian–MCMC excels with those where complementary methods are more appropriate, identify methodological gaps revealed by the case studies (e.g., scalable likelihoods, robust priors, multimodal posterior handling), and outline opportunities for future work in astrophysics and related fields that face similar statistical and computational challenges.

\section{Methodology}
\label{sec:Methodology}

\subsection{Foundations of Bayesian Statistics} % You can also talk about the difference between Bayesian and Frequentist Stats here
The aim of Bayesian statistics is simple: determine $P(H|D)$, or the probability of a hypotheses $H$ being true given data $D$. A hypothesis is any statement that can be true or false, and data is any information that can be used to evaluate the hypothesis. For example, imagine rolling a die. A hypothesis $H$ would that the die roll is a 3. The data $D$ is the result of the die roll.

Hypotheses live in the \textit{hypothesis space}, which is the set of all possible hypotheses of a system \citep{brewer1BayesianInference2018}. Going back to the die example, the hypothesis space is $\{1,2,3,4,5,6\}$. The hypothesis space will also have a probability distribution, or a \textit{prior}, written as $P(H)$. The prior is the probability of each hypothesis being true before seeing any data. For a fair die, the prior is uniform. It's $\frac{1}{6}$ for all $H$ in the hypothesis space. In other words, $P(H)$ is the probability of the hypothesis being true.

Like the prior, the evidence is the probability of seeing the data $D$. The evidence lives in the \textit{data space}, which is the set of all possible data of a system. In the die example, the data space is also $\{1,2,3,4,5,6\}$. The data $D$ also has a probability distribution called the \textit{evidence}, $P(D)$ \citep{brewer1BayesianInference2018}. The evidence is the probability of seeing the data before knowing anything about the hypothesis. In the die example, if you roll a 3, then $P(D)$ is $\frac{1}{6}$ because there is a $\frac{1}{6}$ chance of rolling a 3 on a fair die.

A \textit{likelihood}, $P(D|H)$, is the probability of seeing the data assuming that the hypothesis is true. In the die example, if $H$ is that the die roll is a 3, then $P(D|H)$ is 1 if the die roll is a 3 and 0 otherwise. The likelihood is a function of the hypothesis, not the data. It tells you how likely you are to see the data if a hypothesis is true.

Finally, you can use the prior and likelihood to calculate $P(H|D)$, the probability of a hypothesis being true given the data. This is called the \textit{posterior}. In the die example, if you roll a 3, then $P(H|D)$ is 1 if $H$ is that the die roll is a 3 and 0 otherwise. The posterior is a function of both the hypothesis and the data. It tells you how likely a hypothesis is to be true given the data.

The framework for finding $P(H|D)$ is called Bayes' Theorem, and if forms the center of Bayesian inference. It can be derived using two rules \citep{coxProbabilityFrequencyReasonable1946}. Firstly, the probability that a hypothesis is true and the probability that it is not true add up to 1:

\begin{equation}
  P(H) + P(\tilde{H}) = 1.
\end{equation}

The second rule is the product rule, which states:

\begin{equation}
  P(H)P(D|H) = P(D)P(H|D).
\end{equation}

This can be trivially rearranged to give Bayes' Theorem:

\begin{equation}
  P(H|D) = \frac{P(H)P(D|H)}{P(D)}.
\end{equation}

In other words, given the prior, evidence, and the likelihood, you can calculate the posterior $P(H|D)$.

\subsubsection{Example: The Double-Headed Coin}
The previous example of a die roll was trivial. The data, the result of the die roll, completely determined the hypothesis. Bayesian statistics becomes more useful when the data is incomplete, as is often the case in Astrophysics.

To demonstrate this, consider the following setup. You have 5 coins, four of which are fair, and one of which is double-headed. You pick a coin at random and flip it, and it lands heads. \textit{What is the probability that you picked the double-headed coin?}

The first step is to determine $H$ and $D$ from the hypotheses and data spaces. The hypothesis space is $\{\text{Picked Fair},\text{Picked Double-Headed}\}$. For the sake of conciseness, it can be written as $\{\text{fair},\text{double}\}$. For this problem, we hypothesize that the double-headed coin was picked. So, $H$ is 'double'. The data space is $\{\text{heads},\text{tails}\}$, and for this problem, $D$ is 'heads'.

Next, we determine the prior and evidence. There are 4 fair coins and 1 double-headed coin, so it is easy to find for the prior that $P(\text{fair}) = \frac{4}{5} = 0.8$ and $P(\text{double}) = \frac{1}{5} = 0.2$. The evidence is more complex to find in this case. Since the chance of flipping heads or tails includes the case that you picked up the double headed coin, the evidence cannot simply be 50-50. We must calculate the mean probability of getting heads and tails across all the coins:

\begin{align*}
  P(\text{heads}) &= \frac{0.5+0.5+0.5+0.5+1}{5} \\ &= 0.6 \\ \\
  P(\text{tails}) &= \frac{0.5+0.5+0.5+0.5}{5} \\ &= 0.4 \\
\end{align*}

Note the implicit rule for any space:

\begin{equation}
  \sum_n P(x_n) = 1,
\end{equation}

where $x_n$ is a value in a space.

The final step before solving is to find the likelihood, $P(\text{heads}|\text{double})$. This is trivially 1, as you can only get heads from the double-headed coin. Now, we can solve for the posterior:

\begin{align*}
  P(\text{double}|\text{heads}) &= \frac{P(\text{double})P(\text{heads}|\text{double})}{P(\text{heads})} \\
  &= \frac{0.2}{0.6} = \boxed{0.\bar{3}}
\end{align*}

Before incorporating the data, the probability of picking the double headed coin was $0.2$, but by using Bayesian statistics, we were able to "learn" from the data and increase the probability to $0.\bar{3}$. This is a simple example, but it demonstrates the power of Bayesian statistics in the face of incomplete data.

\subsection{Bayesian Statistics in Astrophysics}

\subsection{Monte Carlo Methods}
\subsubsection{High-Dimensional Integration}
\subsubsection{Basic Limitations}

\subsection{Markov Chain Monte Carlo Theory}
\subsubsection{Markov Chains}
\subsubsection{The Metropolis Algorithm}
\subsubsection{Advanced MCMC}

\subsection{Implementations}
\subsubsection{Software Ecosystem}
\subsubsection{Common Challenges in Astrophysics}



\bibliography{references}

\end{document}
