\documentclass[preprint,longauthor]{aastex631}

\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{etoolbox}

\makeatletter
\pretocmd{\@footnotetext}{\hbadness=10000 \hfuzz=1pt}{}{}
\makeatother

\numberwithin{equation}{section}

\lstdefinestyle{pythonstyle}{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{teal!70!black}\itshape,
  stringstyle=\color{orange!85!black},
  numberstyle=\tiny\color{gray},
  numbers=left,
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!5},
  frame=tb,
  rulecolor=\color{black!30},
  breaklines=true,
  showstringspaces=false,
  tabsize=2,
  captionpos=b
}

% Note to self: Don't forget to start up Zotero when you start writing!

\begin{document}
\label{placeholder} % Delete this when done!

\title{Bayesian Inference and MCMC Methods in Astrophysics}
\author{Agastya Gaur}
\affiliation{University of Illinois at Urbana-Champaign}

\begin{abstract}
  \blindtext
\end{abstract}

\keywords{Astrophysics, Astrostatistics, Bayesian Statistics, Markov Chain Monte Carlo, Big Data}

\section{Introduction}
\label{sec:Introduction}
\subsection{The History of Astrostatistics}
In the 4th century BC, Hipparchus, attempting to estimate the length of a year, found the middle of the range of a scattered set of Babylonian solstice measurements \citep{feigelsonStatisticalChallengesModern2004}. An achievement for the time, Hipparchus's measurement marked the beginning of what would become a long-standing marriage between astronomy and statistics. In the centuries to come, a number of breakthroughs in astrostatistics followed. Notably, Tycho Brahe in the late 1500s made repeated positional measurements of stars using naked-eye observations. The data were so precise that their modeling led to Kepler's new laws of planetary motion \citep{leavesleyTychoBrahesWay2018}. Furthermore, in the 1770s, Laplace rediscovered Bayesian statistics, and over the next decade he expanded upon the theory, using it to modernize Newton's theory of gravity \citep{stiglerStudiesHistoryProbability1975}.

The biggest advance in astrostatistics before the era of computing was in 1805 when Legendre published the method of least squares regression to model the orbit of comets \citep{feigelsonStatisticalChallengesModern2004}. He theorized that the model best fit to a set of data was one that minimized the sum of the squares of the errors. Though Legendre did not provide a formal proof of the method, regarding it only as a convenient trick, later works by Robert Adrain developed formal mathematical proofs of the method \citep{merrimanHistoryMethodLeast1877}. In 1809, Gauss published his own work on least squares, using it to calculate the orbit of the dwarf planet Ceres. Controversially, he also insisted that he had discovered the method years before Legendre \citep{stiglerGaussInventionLeast1981}. Through its impact, least squares regression has cemented itself in history as one of the most important leaps in astrostatistics.

The recurring theme is clear: progress in astronomy often hinges on solving problems of statistical estimation. By the end of the century, astronomy had firmly established itself as a quantitative science, driven by the refinement of statistical methods to identify regularities in scattered measurements, fitting orbital models, and quantifying uncertainty in the presence of noise.

\subsection{The Impact of Astrophysics}

Later years brought two developments that reshaped the relationship between astronomy and statistics: the rise of physics as the foundation of astronomy and the advent of computing, which enabled unprecedented scales of data analysis. As astronomy grew increasingly intertwined with the theories of physics, the field transformed into what we now call astrophysics. Though a niche field called statistical astronomy persisted, the majority of astronomers made insufficient use of statistics in their work \citep{feigelsonStatisticalChallengesModern2004}. The focus shifted to deriving physical models from first principles, and statistical methods were often seen as secondary or even unnecessary. \citet{hubbleDistributionLuminosityElliptical1930} determined the fit for the light curve of elliptical galaxies by trial-and-error instead of regression. \citet{zwickyMassesNebulaeClusters1937} first observed dark matter using a curve fitted only by eye.

The disdain for statistics stemmed from most astrophysicists' stubborn adherence to Newtonian determinism. Physics was regarded as the fundamental law of nature and an elegant basis for astronomy, while statistics was seen as rough, approximate, and imperfect. Statistics also flourished in the social sciences, further alienating it from astrophysics.

However, statistics would not be kept away from astronomy for long. As computing machines developed, astronomers increasingly adopted new tools for both calculation and simulation. In the 1920s, one of the earliest applications appeared in the production of lunar tables. Previously, astronomers calculated the position of the Moon using complex, error-prone methods that required extensive manual computation \citep{duncombeEarlyApplicationsComputer1988}. By the 1930s, however, \citet{comrieApplicationHollerithTabulating1932} demonstrated how punch card computing machines could automate the process, making lunar ephemerides faster and more reliable. From then on to the 1970s, Comrie's work was continued by Wallace Eckert, who improved the punch-card calculations using IBM computers \citep{olleyTaskThatExceeded2018}. Like lunar calculations, galactic simulations were also performed with computational devices as early as the 1940s. \citet{holmbergClusteringTendenciesNebulae1940} modeled gravitational interactions using lightbulbs to represent galaxies, demonstrating how spiral structures could emerge. These analog demonstrations laid the groundwork for digital computer N-body simulations in the 1970s, such as \citet{toomreGalacticBridgesTails1972}, who explained tidal tails and bridges in interacting galaxies.

With these advances in computing came a natural resurgence of statistical methods in astronomy. Given the ability to automate calculations, handle larger datasets, and simulate complex systems, statistical analysis became indispensable. In the mid-20th century, the growth of galaxy surveys encouraged quantitative modeling of structure and dynamics. Early work such as \citet{lynden-bellStatisticalMechanicsViolent1967} applied statistical mechanics to stellar systems, laying the foundation for the study of galaxy formation and equilibrium. By the 1970s, statistics was increasingly recognized as a distinct methodological pillar of astronomy. \citet{peeblesStatisticalAnalysisCatalogs1973} systematically catlogued and analyzed extragalactic objects using power spectra and correlation functions, pioneering the statistical study of large-scale structure.

While the early history of the field was dominated by statistical reasoning and theory, the growth of physics and digital computation broadened this into what we now call quantitative analysis (QA). Quantitative analysis thus represents the merging of three traditions that once stood apart: the deductive rigor of physics, the inferential power of statistics, and the scalability of computation. In modern astronomy, progress often relies not on one of these strands in isolation but on their integration. QA therefore serves as both a methodological framework and a philosophy of practice, emphasizing reproducibility, uncertainty quantification, and the ability to extract physical meaning from complex data.

\subsection{The Data Deluge}

Today, astrophysics sits in a universe of complex statistical problems that demand new quantitative approaches and more computing power with each passing day. These are compounded by an unprecedented era of astronomical data generation in the 21st century. Sky surveys like Gaia DR3 alone provide astrometry and photometry for nearly two billion stars, plus more than ten million variable sources \citep{gaiacollaborationGaiaDataRelease2023}. The nineteenth data release of the Sloan Digital Sky Survey collected spectroscopic data from over 6 million objects \citep{sdsscollaborationNineteenthDataRelease2025}. Advances in CCD detectors will see data from sky surveys increase in the next decade from gigabytes to terabytes today, and possibly to petabytes in the near future. The same trend can be seen in data from NASA's Solar Dynamics Observatory, which now generates over a terabyte of data per day, and the Rubin LSST, generating close to 30 terabytes per day \citep{borneAstroinformatics21stCentury2009}. Compared to the Henry Draper Catalogue \citep{cannonHenryDraperCatalogue1918}—a century-old counterpart that cataloged roughly 200,000 stars—the explosion in data is striking.

The leap from hundreds of thousands of stars in the Henry Draper Catalogue to billions in Gaia represents more than a change in scale: it is a qualitative transformation in what science becomes possible. With the Draper Catalogue, astronomers could classify stellar spectra and trace broad patterns in stellar populations. With Gaia, it is now possible to reconstruct the full three-dimensional structure and kinematics of the Milky Way, identify hypervelocity stars, and test theories of Galactic evolution with unprecedented detail. Where older surveys allowed the identification of a few rare stellar types, modern surveys allow systematic searches for extreme outliers across billions of objects, transforming the statistical character of astronomy.

This data deluge makes QA indispensable. Large-scale surveys now span the entire electromagnetic spectrum, from radio (e.g., LOFAR, ALMA) to X-ray and gamma-ray observatories such as Chandra and Fermi. Multi-messenger astronomy adds yet another layer, with gravitational waves detected by LIGO/Virgo and high-energy neutrinos from IceCube \citep{abbasiSearchIceCubeSubTeV2023}. Time-domain surveys such as ZTF and the upcoming LSST produce streams of transient and variable sources, producing large data volumes and data rates. Each dataset has distinct noise properties, resolutions, and systematic biases, making multi-wavelength integration a formidable statistical task. The ability to extract meaningful insights from these massive datasets is crucial for advancing our understanding of the universe.

\subsection{Statistical Challenges in Modern Astrophysics}

Two recurring types of statistical challenges emerge across astrophysics. The first challenge is that noisy, incomplete, and often degenerate data have uncertain theoretical statistical distributions. In addition, the number, complexity, and degeneracy of physical parameters poses major challenges \citep{schaferFrameworkStatisticalInference2015}. For example, in exoplanet studies, radial velocity measurements can only determine a planet’s minimum mass because they do not contain orbital inclination \citep{lovisRadialVelocityTechniques2010}. In cosmology, measurements of the cosmic microwave background couple the effects of the Hubble constant, the amount of ordinary matter, and dark energy, so isolating any one factor requires prior assumptions about the others \citep{christensenParameterEstimationGravitational2022}. Even within galaxies, rotation curve studies must weight the balance between visible stars and invisible dark matter. Each of these cases requires careful statistical treatment to avoid misleading conclusions.

The second challenge is that the large volume of data and model complexity creates equally daunting problems of computing time and power. As noted above, the Rubin Observatory LSST will generate tens of terabytes of imaging data per night \citep{borneAstroinformatics21stCentury2009}, while Gaia has already released petabyte-scale catalogs \citep{gaiacollaborationGaiaDataRelease2023}. Brute-force exploration of parameter spaces is simply impossible at these scales. Efficient algorithms and scalable statistical methods are required to render analysis computationally tractable \citep{huijseComputationalIntelligenceChallenges2014}. Together, these issues create a need for QA frameworks that can handle degeneracy in complex parameter spaces, model complexities, and scale efficiently with massive datasets.

\subsection{Bayesian Inference and MCMC in Context}

In this domain, Bayesian inference via Markov Chain Monte Carlo (MCMC) methods naturally emerges as a potential solution. Bayesian inference offers a principled framework for parameter estimation in complex systems. MCMC methods provide an effective way to explore parameter spaces by sampling from posterior distributions. This has become one of the most widely used and versatile approaches \citep{vontoussaintBayesianInferencePhysics2011}. Computational models are becoming far more complex, and the data being analyzed is often noisy and incomplete. Bayesian methods, with their ability to incorporate prior parameter constraints and handle uncertainty, are particularly well-suited to these challenges. MCMC methods, in particular, provide a practical way to sample from complex posterior distributions that arise in Bayesian analysis. This makes them invaluable for parameter estimation, model comparison, and uncertainty quantification.

%Flesh out this paragraph more
Another reason for the appeal of Bayesian methods is their contrast with frequentist approaches. Frequentist methods, which were dominant through much of the 20th century, emphasize point parameter estimates and confidence intervals derived from repeated sampling arguments \citep{trottaBayesSkyBayesian2008}. Bayesian inference, in contrast, provides full posterior probability distributions for parameters, naturally incorporating prior information from physics or earlier observations. This framework is particularly powerful where data are sparse, noisy, and incomplete.

The wider adoption of Bayesian statistics in astronomy gained momentum in the late 20th century. Cosmologists have applied Bayesian methods to the cosmic microwave background to extract cosmological parameters from noisy sky maps \citep{tegmarkKarhunenLoeveEigenvalueProblems1997}. In exoplanet science, Bayesian inference became common in the 1990s and 2000s for modeling radial velocity curves and transit signals in the era of larger and more precise datasets \citep{gregoryBayesianAnalysisExtrasolar2005}. Pulsar timing, supernova cosmology, and gravitational lens modeling similarly saw wider use of Bayesian methods. The trend reflects a broader recognition that many of astronomy’s hardest problems demand not just point estimates, but principled uncertainty quantification.

We review here Bayesian inference and MCMC within an astrophysical perspective because of their flexibility, principled uncertainty quantification, and growing ubiquity across astrophysics. In \hyperref[sec:Methodology]{Section II}, the methodology section develops a working foundation: here we review Bayesian statistics, priors, and likelihood construction in realistic astronomical settings. We cover toy examples before escalating to domain-relevant formulations. Then, we introduce Monte Carlo methods and build to Markov Chain Monte Carlo, deriving the Metropolis family and related samplers, providing step-by-step Python implementations on simple problems.

The next three sections present focused case studies that illustrate how Bayesian–MCMC pipelines advance frontiers in different subfields. \hyperref[sec:CaseStudies_DirectImaging]{Section III} outlines the observational context and challenges facing the direct detection of exoplanets, such as disentangling planetary signals from stellar activity, classifying threshold for non-detections, and classifying imaged exoplanets. It then outlines the methodology and advantages of Bayesian solutions to these problems. \hyperref[sec:CaseStudies_CMB]{Section IV} covers CMB parameter estimation, considering geometric degeneracies in the power spectrum. It explores how parallelizable frameworks for MCMC and Bayesian treatments of neural networks enable efficient posterior calculations, including tradeoffs between sampler sophistication and wall-clock efficiency. Finally, \hyperref[sec:CaseStudies_Other]{Section V} covers a non-exhaustive set of other astrophysical applications, including gravitational wave detection, dark matter searches, and black hole modeling.

Finally, in \hyperref[placeholder]{Section VI} and \hyperref[placeholder]{VII} we compare cases where Bayesian–MCMC excels with those where complementary methods are more appropriate, identify methodological gaps revealed by the case studies, and outline opportunities for future work in astrophysics and related fields that face similar statistical and computational challenges.

\section{Methodology}
\label{sec:Methodology}

\subsection{Bayesian Preliminaries} % You can also talk about the difference between Bayesian and Frequentist Stats here
The aim of Bayesian statistics is to determine $P(H|D)$, or the probability of a hypotheses $H$ being true given data $D$. A hypothesis is any statement that can be true or false, and data is any information that can be used to evaluate the hypothesis. For example, for the roll of a single die, an hypothesis $H$ would that the die roll is a 3. The data $D$ is the result of the die roll.

Hypotheses live in the \textit{hypothesis space}, which is the set of all possible hypotheses of a system \citep{brewer1BayesianInference2018}. For the die example, the hypothesis space is $\{1,2,3,4,5,6\}$. The hypothesis space will also have a probability distribution, or a \textit{prior}, written as $P(H)$ \citep{brewer1BayesianInference2018}. The prior is the probability of each hypothesis being true absent any data. For a fair die, the prior is uniform: $\frac{1}{6}$ for all $H$ in the hypothesis space. In other words, $P(H)$ is the probability of the hypothesis being true.

The data $D$ also has a probability distribution called the \textit{evidence}, $P(D)$ \citep{brewer1BayesianInference2018}. The evidence is the probability of the data absent any hypothesis. In the die example, $P(D)$ is $\frac{1}{6}$ for rolling a $3$ on a fair die. The evidence lives in the \textit{data space}, which is the set of all possible data outcomes. In the die example, the data space is also $\{1,2,3,4,5,6\}$.

A \textit{likelihood}, $P(D|H)$, is the probability of the data assuming that the hypothesis is true \citep{brewer1BayesianInference2018}. In the die example, if $H$ is that the die roll is a $3$, then $P(D|H)$ is unity if the die roll is a $3$ and zero otherwise. We use the prior, evidence, and likelihood to calculate $P(H|D)$, which is the probability of a hypothesis being true given the data. This is called the \textit{posterior} \citep{brewer1BayesianInference2018}. In the die example, if you roll a $3$, then $P(H|D)$ is unity if the hypothesis $H$ is that the die roll is a $3$. It is zero otherwise.

The posterior $P(H|D)$ is defined by Bayes' Theorem. It can be derived using two rules, as outlined in \citet{coxProbabilityFrequencyReasonable1946}. Firstly, the probability that a hypothesis is true $P(H)$ and the probability that it is not true $P(\tilde{H})$ add up to unity:

\begin{equation}
  P(H) + P(\tilde{H}) = 1.
\end{equation}

The second rule is the product rule for conditional probabilities, which states:

\begin{equation}
  P(H)P(D|H) = P(D)P(H|D).
\end{equation}

This can be rearranged to give Bayes' Theorem:

\begin{equation}
  P(H|D) = \frac{P(H)P(D|H)}{P(D)}.
  \label{eq:bayes_theorem}
\end{equation}

\subsubsection{Example: The Double-Headed Coin}
For the previous example of a die roll, the data, the result of the die roll, completely determined the hypothesis. Bayesian statistics becomes more useful when the data are incomplete, as is often the case in astrophysics.

Consider five coins, four of which are fair, and one of which is double-headed. A random coin lands heads. We calculate the probability that the double headed coin was randomly selected.

The first step is to determine $H$ and $D$ from their respective spaces. The hypothesis space is $\{\text{Picked Fair},\text{Picked Double-Headed}\}$, which can be written concisely as $\{\text{fair},\text{double}\}$. We hypothesize that the double-headed coin was picked ($ H = \text{'double'}$). The data space is $\{\text{heads},\text{tails}\}$, and $D$ is 'heads'.

For four fair coins and one double-headed coin the priors are: $P(\text{fair}) = \frac{4}{5} = 0.8$ and $P(\text{double}) = \frac{1}{5} = 0.2$. The evidence is more complex to find in this case. Since the chance of flipping heads or tails includes the case that the double headed coin was picked, the evidence cannot be 50-50. We must calculate the probability of getting heads and tails across all the coins:

\begin{align*}
  P(\text{heads}) &= \frac{4(0.5)+1(1)}{5} = 0.6 \\
  P(\text{tails}) &= \frac{4(0.5)+1(0)}{5} = 0.4
\end{align*}

Note the implicit rule for any probability space:

\begin{equation}
  \sum_n P(x_n) = 1,
\end{equation}

where $x_n$ is a value in the space.

The likelihood $P(\text{heads}|\text{double})$ is unity, as it is only possible to get heads from the double-headed coin. Using Bayes' Theorem (\autoref{eq:bayes_theorem}):

\begin{align*}
  P(\text{double}|\text{heads}) &= \frac{P(\text{double})P(\text{heads}|\text{double})}{P(\text{heads})} = \frac{0.2}{0.6} = \boxed{0.\bar{3}}
\end{align*}

Before incorporating the data, the probability of picking the double headed coin was $0.2$, but by using Bayesian statistics, including information from the data increases the probability to $0.\bar{3}$. This example demonstrates the power of Bayesian statistics for incomplete data.

\subsection{Bayesian Statistics in Astrophysics}

In an astrophysical context, Bayes' Theorem is used to calculate the probability of \textit{parameters}, $\theta$, of a model rather than a hypothesis \citep{brewer1BayesianInference2018}.

\begin{equation}
  P(\theta|D) = \frac{P(\theta)P(D|\theta)}{P(D)}.
  \label{eq:bayes_theorem_params}
\end{equation}

Consider measurements of the radial velocity of a star over time. When a start hosts an orbiting planet, both bodies orbit a common center of mass. This induces a wobble in the star's motion which is detected as periodic Doppler shifts in the star's spectrum, also known as the \textit{radial velocity}. The radial velocity $v_r(t)$ of a star with a single planet in a strictly circular orbit can be modeled by the following equation \citep{lovisRadialVelocityTechniques2010}:

\begin{equation}
  v_r(t) = K \sin\left(\frac{2\pi t}{T} + \phi\right),
  \label{eq:rv_model}
\end{equation}

where $K$ is the velocity semi-amplitude, $T$ is the orbital period, and $\phi$ is an orbital phase offset. Thus, the parameters of this model are $\theta = \{K,P,\phi\}$. This is simplified to omit the parameters for eccentricity, system velocity, and argument of periapsis.

Consider the simulated data for radial velocity of a star shown in \autoref{fig:rv_data}. Bayes' Theorem (\autoref{eq:bayes_theorem_params}) becomes:

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{../scripts/2.2/figures/data.png}
\caption{Simulated radial velocity data of a star with a single orbiting planet (\autoref{eq:rv_model}). Each point has an error of three standard deviations. The data are noisy and incomplete, making it difficult to determine the parameters of the model.}
\label{fig:rv_data}
\end{figure}

\begin{equation}
  P(K,T,\phi|D) = \frac{P(K,T,\phi)P(D|K,T,\phi)}{P(D)}.
\end{equation}

Assuming for simplicity that the parameters are independent, the prior can be written as:

\begin{equation}
  P(K,T,\phi) = P(K)P(T)P(\phi).
\end{equation}

Absent any information, we assume the priors are uniformly distributed in a certain range. From inspection, the velocity semi-amplitude $K$ is between 5 and 15 m/s, the period $T$ is between 20 and 30 days, and the phase offset $\phi$ is between 0 and $2\pi$. Thus, the adopted uniform priors are:

\begin{equation*}
  P(K) = \begin{cases}
    \frac{1}{10} & 5 < K < 15 \\
    0 & \text{otherwise}
  \end{cases} \qquad
  P(T) = \begin{cases}
    \frac{1}{10} & 20 < T < 30 \\
    0 & \text{otherwise}
  \end{cases} \qquad
  P(\phi) = \begin{cases}
    \frac{1}{2\pi} & 0 < \phi < 2\pi \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}

and the combined prior is then:

\begin{equation}
  P(K,T,\phi) = \begin{cases}
    \frac{1}{200\pi} & 5 < K < 15, 20 < T < 30, 0 < \phi < 2\pi \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

The likelihood, $P(D|K,T,\phi)$, is the probability of the data given the parameters. For a given set of parameters, the model prediction is the mean of a normal distribution with standard deviation equal to the error of each data point. This provides the probability $P_i$ of obtaining a data point $d_i$ at time $t_i$. The likelihood for the full dataset is:

\begin{equation}
  P(D|K,T,\phi) = \prod_{i=1}^{N} P_i
\end{equation}

This takes the form:

\begin{equation}
  P(D|K,T,\phi) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left(-\frac{(D_i - v_r(t_i;K,T,\phi))^2}{2\sigma_i^2}\right)
\end{equation}

As a probability, the posterior integrated over the entire parameter space must equal unity. Thus, the evidence $P(D)$ can be found as follows:

\begin{align}
  1 &= \iiint P(K,T,\phi|D) dK dT d\phi\nonumber \\
  1 &= \iiint \frac{P(K,T,\phi)P(D|K,T,\phi)}{P(D)} dK dT d\phi \nonumber \\
  \therefore P(D) &= \iiint P(K,T,\phi)P(D|K,T,\phi) dK dT d\phi
\end{align}

This integral is analytically intractable, so it must be calculated numerically.

Using the data in \autoref{fig:rv_data}, the posterior distribution can be calculated using the code in \autoref{appx:A}. The code uses the log-prior and log-likelihood to avoid any underflow errors. The posterior can be calculated by adding the log-prior and log-likelihood, and then exponentiating the result, then normalizing with the evidence.

\begin{equation}
  P(K,T,\phi|D) = \frac{e^{\log(P(K,T,\phi)) + \log(P(D|K,T,\phi))}}{P(D)}
\end{equation}

The program evaluates the posterior on a grid of 100 points for each parameter, giving a total of $100^3 = 1,000,000$ points. The posterior is also cumulatively summed at every evaluated point, giving a numerical estimate of the evidence $P(D)$. Finally, the marginal posteriors of each parameter are calculated from the total posterior by integrating over the other two parameters:
\begin{align}
  P(K|D) &= \iint P(K,T,\phi|D) dT d\phi \\
  P(T|D) &= \iint P(K,T,\phi|D) dK d\phi \\
  P(\phi|D) &= \iint P(K,T,\phi|D) dK dT
\end{align}
The resulting posterior distributions are shown in \autoref{fig:bf_posteriors}.

\begin{figure}[ht!]
\centering

\subfigure[$\mu = 9.99$ m/s $\quad \sigma = 0.73$ m/s]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/k-posterior.png}%
    \label{fig:k_posterior}%
}
\hfill
\subfigure[$\mu = 21.72$ days $\quad \sigma = 0.44$ days]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/t-posterior.png}%
    \label{fig:t_posterior}%
}
\hfill
\subfigure[$\mu = 0.89$ rad $\quad \sigma = 0.13$ rad]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/phi-posterior.png}%
    \label{fig:phi_posterior}%
}
\caption{The marginal posterior distributions of the parameters $K$, $T$, and $\phi$ given the data in \autoref{fig:rv_data}. These posteriors are normalized with a numerically calculated evidence.}
\label{fig:bf_posteriors}
\end{figure}

The exact parameters of a model are always unknown, but posterior distributions field a close estimate. The true parameters, $K=10$ m/s, $T=21.4$ days, and $\phi=\frac{\pi}{4}$, fall within one standard deviation of the mean of each posterior distribution (\autoref{fig:bf_posteriors}), indicating that the Bayesian method was successful in estimating the parameters.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{../scripts/2.2/figures/prediction.png}
\caption{The radial velocity data from \autoref{fig:rv_data} along with the mean prediction from the posterior distributions in \autoref{fig:bf_posteriors}. The shaded region represents one standard deviation from the mean prediction.}
\label{fig:prediction}
\end{figure}

\autoref{fig:prediction} shows the true model and the mean parameter prediction from the posterior distributions. Bayes' Theorem is thus useful to estimate the parameters for astrophysical models. However, this method is computationally expensive. Computing the posterior numerically requires integrating over the full parameter space. For $n$ parameters and $m$ possible values for each parameter, this algorithm would have a time complexity of $O(m^n)$, which is infeasible for even small $n$ and $m$. To solve this problem, we can use \textit{Monte Carlo methods} \citep{brewer1BayesianInference2018}.

\subsection{Monte Carlo Sampling}
As noted above, the primary computational challenge in brute force Bayesian inference arises from evaluating the posterior probability at every point in the parameter space. One way to reduce this cost is to sample only a subset of points, distributed according to the posterior itself \citep{vontoussaintBayesianInferencePhysics2011}. These points are called \textit{draws}. These draws make it possible to calculate expectation values from the explicit posterior using only the sampled points \citep{vontoussaintBayesianInferencePhysics2011}. This defines \textit{Monte Carlo sampling}, which can be used to approximate integrals and expectations that would otherwise require explicit evaluation over a continuous space.

For a known probability density, the expectation value of some variable $x$ can be calculated as:

\begin{equation}
  \langle x \rangle = \int x P(x|\theta) dx.
\end{equation}

For $N$ values of $x$ drawn from $P$ using Monte Carlo, this can be discretely approximated as:

\begin{equation}
  \langle x \rangle \approx \frac{1}{N} \sum_{i=1}^{N} x_i,
\end{equation}
which generalizes naturally to higher-dimensional parameter spaces, though convergence can become slower as dimensionality increases \citep{vontoussaintBayesianInferencePhysics2011}.

$x$ is a function of the parameters, and can be defined arbitrarily to calculate the expectation value of any value of interest. Given a probability density $P(\theta_1,\theta_2,\hdots,\theta_n)$, some forms of $x$ are listed in \autoref{tab:x_forms}.
\begin{deluxetable}{ll}
  \tablewidth{0pt}
  \tablenum{1}
  \tablehead{
    \colhead{$x$} & \colhead{Description}
  }
  \startdata
    $\theta_i$ & Expectation value of parameter $\theta_i$ \\
    $\theta_i^2$ & Variance of parameter $\theta_i$ \\
    $f(\theta_1,\theta_2,\hdots,\theta_n)$ & Expectation value of any function of the parameters \\
    $\mathds{1}(\theta_i = a)$ & Probability that $\theta_i = a$ \\
    $\mathds{1}(a \le \theta_i \le b)$ & Probability that $\theta_i$ is in the range $[a,b]$ \\
    $\mathds{1}(\text{condition})$ & Probability that the condition is true \\
  \enddata
  \caption{Forms of $x$ to calculate expectation values of various quantities.}
  \label{tab:x_forms}
\end{deluxetable}

The usefulness of Monte Carlo sampling becomes apparent when considering the computation of marginal posteriors. In the previous example, we used the posterior to calculate the marginal posteriors of each parameter. In the brute force method, this was found by integrating the posterior over all other parameters. Monte Carlo makes this simpler. If we have a set of $N$ draws from the posterior of a parameter space $\{\theta_1,\theta_2,\hdots,\theta_n\}$, we can obtain draws for a certain parameter $\theta_i$ by ignoring all other parameters \citep{brewer1BayesianInference2018}. A histogram of the draws of $\theta_i$ then gives the marginal posterior of $\theta_i$. This is much simpler than integrating over all other parameters, especially in high-dimensional parameter spaces.

\subsection{Markov Chain Monte Carlo}
\textit{Markov Chain Monte Carlo} (MCMC) is one of the many methods used to obtain posterior draws, and in modern astrophysics, has become a standard tool for Bayesian inference. Using MCMC, the posterior of even a high-dimensional parameter space can computed efficiently \citep{trottaBayesSkyBayesian2008}. MCMC does this by creating a sequence of draws in the form of a \textit{Markov Chain} \citep{neal1993probabilistic}.

\subsubsection{Markov Chains}

A Markov Chain is a sequence of random variables, $x_0, x_1, \hdots, x_n$, where the probability of each variable only depends on the previous variable \citep{neal1993probabilistic}. This is known as the \textit{Markov property}, and can be written mathematically as:
\begin{equation}
  P(x_{n+1}|x_n,x_{n-1},\hdots,x_0) = P(x_{n+1}|x_n).
\end{equation}

To create a Markov chain, we must first decide an initial value $x_0$ using an initial probability distribution $P_0(x_0)$. The probability distribution of the next value is evolved iteratively from the previous probability distribution using a \textit{transition probability} \citep{neal1993probabilistic} $T_n(x_n,x_{n+1})$:
\begin{equation}
  P(x_{n+1}) = P(x_n)T_n(x_n,x_{n+1})
\end{equation}

If the transition probability does not depend on the point in the chain, the chain is \textit{homogenous}, or \textit{stationary} \citep{neal1993probabilistic}.

Markov chains can be shown to converge to a \textit{stationary distribution} $P^*(x)$, which is independent of the initial distribution \citep{trottaBayesSkyBayesian2008}. A stationary distribution, once reached, does not change as the chain evolves further.

Markov chains can also be \textit{ergodic} \citep{neal1993probabilistic}, which means that a stationary distribution can be reached from any initial distribution. A ergodic chain is irreducible and aperiodic \citep{neal1993probabilistic}. Irreducibility means that it is possible to reach any point in the space from any other point, and aperiodicity means that the chain does not get stuck in cycles \citep{vontoussaintBayesianInferencePhysics2011}.

For Bayesian inference, the goal of MCMC is to construct a Markov chain whose stationary distribution is the unknown posterior distribution \citep{brewer1BayesianInference2018}. To ensure that the chain converges to the desired posterior, the choice of transition probability $T(x_n, x_{n+1})$ is crucial. The transition rule must satisfy the condition of \textit{detailed balance}, which guarantees that, at equilibrium, the probability flow between any two states is symmetric:
\begin{equation}
  P(x_n)T(x_n, x_{n+1}) = P(x_{n+1})T(x_{n+1}, x_n).
\end{equation}
This condition ensures that the chain does not drift away from the target distribution once it has been reached \citep{vontoussaintBayesianInferencePhysics2011}.

\subsubsection{The Metropolis-Hastings Algorithm}

One of the simplest and most widely used algorithms that satisfies detailed balance is the \textit{Metropolis algorithm} \citep{metropolisEquationStateCalculations1953}, more specifically, its generalization, the \textit{Metropolis–Hastings algorithm} \citep{hastingsMonteCarloSampling1970}. The algorithm constructs a Markov chain by iteratively proposing new states in the parameter space and deciding whether to accept or reject them based on the target distribution. \citet{brewer1BayesianInference2018} outlines the steps of the Metropolis-Hastings algorithm as follows:

\begin{enumerate}
  \item Initialize the chain with a starting point $x_0$.
  \item Generate a candidate point $x_{n+1}$ from a proposal distribution $q(x_{n+1}|x_n)$ based on the current state $x_n$.
  \item Accept or reject the proposal
  \item Repeat steps 2 and 3 for a large number of iterations to generate a sequence of samples until convergence.
\end{enumerate}

The algorithm depends on the proposal distribution $q(x_{n+1}|x_n)$ and the acceptance criterion. Commonly, a 'random walk' proposal is used to generate candidates by adding a small random perturbation to the current state. For an $n$-dimensional parameter space, the random walk proposal is a symmetric (usually Gaussian) distribution, also of $n$-dimensions, with the current state as its mean \citep{vontoussaintBayesianInferencePhysics2011}. If the proposal distribution is indeed symmetric, then the following is implied:
\begin{equation}
  \label{eq:symmetric_proposal}
  q(x_{n+1}|x_n) = q(x_n|x_{n+1}).
\end{equation}
In this case, the Metropolis-Hastings algorithm simplifies to the original Metropolis algorithm \citep{brewer1BayesianInference2018}. Asymmetric proposal distributions are more general, and can also be used. However, they are less common in practice due to their specificity to the problem.

The acceptance criterion is based on the ratio of the posterior at the proposed and current states, adjusted by the proposal distribution. This is the probability of the proposal being accepted. Mathematically, this is \citep{brewer1BayesianInference2018}:
\begin{equation}
  \alpha = \min\left(1, \frac{P(x_{n+1}|D) q(x_n|x_{n+1})}{P(x_n|D) q(x_{n+1}|x_n)}\right) = \min\left(1, \frac{P(D|x_{n+1}) P(x_{n+1}) q(x_n|x_{n+1})}{P(D|x_n) P(x_n) q(x_{n+1}|x_n)}\right).
\end{equation}
If the proposal distribution is symmetric, we can further apply \autoref{eq:symmetric_proposal} to obtain \citep{brewer1BayesianInference2018}:
\begin{equation}
  \alpha = \min\left(1, \frac{P(D|x_{n+1}) P(x_{n+1})}{P(D|x_n) P(x_n)}\right).
\end{equation}
Simplified with log-priors and log-likelihoods:
\begin{equation}
  \log(\alpha) = \min\left(0, \log(P(D|x_{n+1})) + \log(P(x_{n+1})) - \log(P(D|x_n)) - \log(P(x_n))\right).
\end{equation}

This criterion accepts any proposal that increases the posterior probability, while proposals that decrease it are accepted with a probability proportional to the decrease. If a proposal is rejected, the current state is counted again. This allows the chain to sample high-probability regions more frequently while still having the ability to escape local maxima and explore the full parameter space \citep{brewer1BayesianInference2018}.

The choice of proposal distribution can also significantly affect the efficiency of the algorithm \citep{vontoussaintBayesianInferencePhysics2011}. If the proposal steps are too small, the chain will explore the parameter space slowly, leading to high correlation between samples. Conversely, if the steps are too large, many proposals will be rejected, also resulting in inefficient sampling. Tuning the proposal distribution to balance exploration and acceptance rates is often necessary for optimal performance \citep{vontoussaintBayesianInferencePhysics2011}. A common heuristic is to adjust the proposal distribution to achieve an acceptance rate between $0.2$ and $0.5$, depending on the dimensionality of the parameter space \citep{gelmanWeakConvergenceOptimal1997}.

The Metropolis-Hastings algorithm has a \textit{burn-in} period. The initial samples of the chain may not be representative of the target distribution, especially if the starting point is far from high-probability regions. To mitigate this, it is common practice to discard a certain number of initial samples, from the burn-in period, before using the remaining samples for inference \citep{vanravenzwaaijSimpleIntroductionMarkov2018}.

The Metropolis-Hastings algorithm outputs a Markov chain of samples that approximate the target posterior distribution \citep{hastingsMonteCarloSampling1970}. Since the probability is only calculated for proposed points instead of the full parameter space, the algorithm is much more computationally efficient than brute force methods. The time complexity is reduced to $O(n*m)$, where $n$ is the number of samples drawn and $m$ is the number of walks, which is feasible for even high-dimensional parameter spaces.

\subsection{MCMC in Astrophysics}

Revisiting the radial velocity example, we can use MCMC to obtain draws from the posterior distribution of the parameters $\{K,T,\phi\}$ from \autoref{eq:rv_model} given the data in \autoref{fig:rv_data}. Using the Metropolis-Hastings algorithm, 10 chains were run, each with 12,000 samples. The first 1,000 samples of each chain were discarded as burn-in. The code used to run the MCMC is given in \autoref{appx:B}.

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{../scripts/2.5/figures/traces.png}
\caption{Trace plots of the Markov chains for each parameter. A total of 10 chains were run, each with 12,000 samples. The first 1,000 samples of each chain were discarded as burn-in.}
\label{fig:mcmc_trace}
\end{figure}

The trace plots of the chains are shown in \autoref{fig:mcmc_trace}. These are visualizations of the Markov chains for each parameter. The chains appear to be well-mixed and stationary, indicating that they have converged to the target distribution. The marginal posterior distributions of each parameter is shown in \autoref{fig:mcmc_posteriors}.

\begin{figure}[ht!]
\centering

\subfigure[$\mu = 10.02$ m/s $\quad \sigma = 0.73$ m/s]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/k-posterior.png}%
    \label{fig:k_mcmc_posterior}%
}
\hfill
\subfigure[$\mu = 21.71$ days $\quad \sigma = 0.43$ days]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/t-posterior.png}%
    \label{fig:t_mcmc_posterior}%
}
\hfill
\subfigure[$\mu = 0.89$ rad $\quad \sigma = 0.13$ rad]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/phi-posterior.png}%
    \label{fig:phi_mcmc_posterior}%
}
\caption{The histograms of the draws from the posterior distributions of the parameters $K$, $T$, and $\phi$ given the data in \autoref{fig:rv_data}.}
\label{fig:mcmc_posteriors}
\end{figure}

Comparing these posteriors to the ones obtained using the brute-force method, we see that they are very similar. \autoref{tab:comparison} summarizes the comparison between the true parameter values, the estimates from the brute-force method (\autoref{fig:bf_posteriors}), and the estimates from the MCMC method (\autoref{fig:mcmc_posteriors}). The two algorithms are compared using the mean and standard deviation of each posterior distribution, as well as the Z-score of the true value from the mean of each posterior distribution. The Z-score is a measure of how many standard deviations a value is from the mean, and is calculated as:
\begin{equation}
  Z = \frac{X - \mu}{\sigma},
\end{equation}
where $X$ is the value being compared, $\mu$ is the mean of the distribution, and $\sigma$ is the standard deviation of the distribution. A Z-score of zero indicates that the value is equal to the mean, while a Z-score of 1 indicates that the value is one standard deviation above the mean. A Z-score of -1 indicates that the value is one standard deviation below the mean. A Z-score within the range of -2 to 2 is generally considered to be acceptable, as it indicates that the value is within two standard deviations of the mean.
\begin{deluxetable}{llllll}
  \tablewidth{0pt}
  \tablenum{2}
  \tablehead{
    \colhead{Parameter} & \colhead{True} & \colhead{Brute-Force} & \colhead{Brute-Force Z-score} & \colhead{MCMC}  & \colhead{MCMC Z-score}
  }
  \startdata
    $K$ (m/s) & $10.00$ & $9.99 \pm 0.73$ & -0.01 & $10.02 \pm 0.73$ & 0.03 \\
    $T$ (days) & $21.4$ & $21.72 \pm 0.44$ & 0.68 & $21.71 \pm 0.43$ & 0.66 \\
    $\phi$ (rad) & $0.25\pi$ & $0.89 \pm 0.13$ & 0.77 & $0.89 \pm 0.13$ & 0.77 \\
  \enddata
  \caption{Comparison of the true parameter values, the estimates from the brute-force method, and the estimates from the MCMC method.}
  \label{tab:comparison}
\end{deluxetable}

Both methods give very similar results (\autoref{tab:comparison}). The Z-scores of the true values are all within 1, indicating that both methods were successful in estimating the parameters. The MCMC method, however, was far more computationally efficient. As described above, the brute-force method had to evaluate the posterior at $100^3 = 1,000,000$ points in the parameter space. The MCMC method, as implemented in \autoref{appx:B}, only had to evaluate the posterior at $10 \times 12,000 = 120,000$ points, a reduction of nearly an order of magnitude, to provide virtually the same results.

Generalizing this to higher-dimensional parameter spaces, the computational savings of MCMC become even more apparent. In a hypothetical 10-dimensional parameter space with 100 possible values for each parameter, the brute-force method would have to evaluate the posterior at $100^{10} = 10^{20}$ points, which is infeasible. The MCMC method, on the other hand, would still provide reasonably accurate results with no significant increase in computing cost. The number of required samples and walks increases with the number of dimensions based on the discretion of the experimenter. Usually, this increase is sub-exponential, even linear with respect to an $n$-dimensional parameter space.

\subsection{Applications in Research}

Using Bayesian inference and MCMC in astrophysics is far more complex than the radial velocity example given above. Real-world astrophysical models often have many more parameters, complex likelihood functions, and intricate prior distributions. In practice, astrophysical analyses must contend with challenges such as correlated noise in observations, hierarchical modeling of populations, and significant computational bottlenecks when scaling to high-dimensional or large datasets. The adaptability of the Bayesian–MCMC framework makes it useful in these conditions to tackle problems of parameter estimation, model selection, and uncertainty quantification across various astrophysical subfields.

In the following sections, we will explore some case studies where Bayesian inference and MCMC have been successfully applied, including exoplanet detection, cosmological parameter estimation, and gravitational wave data analysis.

\section{Direct Imaging of Exoplanets}
\label{sec:CaseStudies_DirectImaging}

\subsection{Introduction}

Exoplanets are planets that orbit stars outside our solar system, with each being a unique laboratory for studying planetary formation and evolution \citep{kaushikExoplanetDetectionDetailed2025}. Studying their atmospheres, compositions, and orbital dynamics can provide insights into the conditions necessary for life and the diversity of planetary systems in the universe. These properties are key to understanding not only the planets themselves but also the broader processes that govern planetary system formation and evolution \citep{madhusudhanExoplanetaryAtmospheresKey2019}.

Exoplanets are detected using various methods, including transit photometry, radial velocity measurements, direct imaging, and other less common techniques \citep{weiSurveyExoplanetaryDetection2018}. Each method probes a different region of exoplanet parameter space, with transit and radial velocity methods being most sensitive to close-in planets, while direct imaging is sensitive to young, massive planets at wide separations \citep{fischerExoplanetDetectionTechniques2014b}. Of these, direct imaging is the most straightforward in concept, as it does not rely on indirect signatures. The brightness of an exoplanet, whether reflected starlight or thermal emission, is directly imaged by a telescope \citep{kaushikExoplanetDetectionDetailed2025}. This approach enables direct spectroscopic analysis, offering unique access to the planet’s atmospheric composition, temperature, and cloud structure. Although the yield of direct imaging is lower than that of indirect methods, directly imaged exoplanets constitute a crucial subset of targets for atmospheric and dynamical characterization \citep{currieDirectImagingSpectroscopy2023}.

Direct imaging has achieved several notable successes since its first confirmed detections in the early 2000s. The HR 8799 system, imaged by \citet{maroisDirectImagingMultiple2008}, revealed four massive exoplanets orbiting a young A-type star, marking a milestone in multi-planet imaging. Other examples include $\beta$ Pictoris b \citep{lagrangeGiantPlanetImaged2010} and 51 Eridani b \citep{macintoshDiscoverySpectroscopyYoung2015}, both of which have become benchmark systems for understanding the atmospheres of young gas giants. More recently, the James Webb Space Telescope (JWST) \textbf{[ADD FOOTNOTE]} and ground-based instruments such as SPHERE \textbf{[ADD FOOTNOTE]} on the Very Large Telescope (VLT) \textbf{[ADD FOOTNOTE]} and GPI \textbf{[ADD FOOTNOTE]} on Gemini South have achieved higher contrasts and enabled spectroscopy of fainter, cooler companions such as HIP 65426 b \citep{carterJWSTEarlyRelease2023}.

However, direct imaging faces substantial observational and technical challenges. The primary limitation arises from the extreme contrast between the stellar and planetary brightness: even a bright exoplanet is typically $10^{9}$ times fainter than its host star \citep{fischerExoplanetDetectionTechniques2014b}. A dominant obstacle is also the \textit{point spread function} (PSF), which is the diffraction pattern created by the telescope optics \citep{fischerExoplanetDetectionTechniques2014b}. The PSF spreads starlight over the detector and can easily obscure faint companions \citep{fischerExoplanetDetectionTechniques2014b}. PSFs come in various shapes, ranging from circular Airy rings to more complex forms depending on the specifics of the telescope aperture \citep{fischerExoplanetDetectionTechniques2014b}. Various image processing techniques are employed to enhance the visibility of the exoplanet against the stellar background. These techniques can be broadly categorized into \textit{preprocessing} and \textit{postprocessing} methods.

\subsubsection{Preprocessing methods}

Preprocessing methods aim to reduce the intensity of the central PSF and improve the contrast between the star and the exoplanet. This was first performed using a \textit{coronagraph} \citep{lyotStudySolarCorona1939}. This optical device is placed in the light path of the telescope to block out the light from the central star, allowing the much dimmer light from surrounding objects, such as exoplanets, to become more promiment. This reduces the intensity of the central star by up to 2 orders of magnitude \citep{chauvinDirectImagingExoplanets2023}.

Changing the shape of the telescope aperture can also advantageously change the shape of the PSF. \citet{zanoniReductionDiffractedLight1965} argue for a square aperture, thereby constaining the PSF to the \textit{x} and \textit{y}-axes and suppressing the diagonals, which reduce the intensity of diffracted light from the central star by four orders of magnitude.

Modern preprocessing methods include \textit{apodization}, which is a technique to reduce the brightness of the PSF without blocking out the host star. This is accomplished by altering the shape and transmission of a telescope aperture \citep{nisensonDetectionEarthlikePlanets2001}. A number of apodization techniques are used, ranging from pupil-plane masks \citep{reddyApodizationPupilsDesign2018}, to phase-induced approaches \citep{guyonExoplanetImagingPhaseinduced2005}.

\subsubsection{Postprocessing methods}

Once an image has been acquired, postprocessing methods play a crucial role in distinguishing faint planetary signals from residual starlight. These techniques operate on the recorded data to remove the effect of the PSF, revealing features masked by the intense starlight \citep{lafreniereNewAlgorithmPoint2007}. A simple approach is \textit{reference differential imaging} (RDI), which involves taking images of a reference star with similar properties to the target star. The reference image is then subtracted from the target image to remove the stellar contribution \citep{folletteIntroductionHighContrast2023}. However, in the presence of temporal variations in the atmosphere and instrument, RDI may not accurately capture the PSF variations, leading to residual artifacts.

To account for this, more advanced techniques such as \textit{angular differential imaging} (ADI) \citep{maroisAngularDifferentialImaging2006} and \textit{spectral differential imaging} (SDI) \citep{folletteIntroductionHighContrast2023} have been developed. ADI leverages the rotation of the field of view during observations to distinguish between static PSF features and rotating planetary signals \citep{maroisAngularDifferentialImaging2006}. SDI, on the other hand, exploits the spectral differences between the star and planet by observing at multiple wavelengths, allowing for differential subtraction based on color contrasts \citep{folletteIntroductionHighContrast2023}.

In addition to these methods, statistical PSF modeling techniques have become widely adopted. Locally Optimized Combination of Images (LOCI) \citep{lafreniereNewAlgorithmPoint2007} constructs a linear combination of reference frames that minimizes residual noise within localized regions of the image. Principal Component Analysis (PCA) or Karhunen-Loève Image Projection (KLIP) extended this concept, representing the PSF as a combination of orthogonal basis images derived from large reference libraries \citep{folletteIntroductionHighContrast2023}.

\subsection{Challenges}

Even after filtering out the stellar emission, several challenges remain in the direct imaging of exoplanets. One major challenge is the presence of \textit{speckle noise}, which arises from residual wavefront errors (microscopic imperfections in the telescope aperture) and residual refractive atmospheric errors \citep{fischerExoplanetDetectionTechniques2014b}. These errors create discrete speckles that can mimic the appearance of point sources, making it difficult to distinguish between true planetary signals and noise artifacts \citep{fischerExoplanetDetectionTechniques2014b}. Furthermore, overfiltering during PSF subtraction can lead to \textit{self-subtraction}, where part of the planet's signal is inadvertently removed along with the stellar PSF, leading to \textit{non-detections} \citep{kaushikExoplanetDetectionDetailed2024}. This effect is particularly pronounced for planets close to their host stars.

Furthermore, spectroscopy of directly imaged exoplanets requires significant observational time to classify planetary features \citep{ruaneDeepImagingSearch2017, bixelIdentifyingExoEarthCandidates2019}. This is further exacerbated by the need to observe multiple spectral bands to capture the full range of atmospheric features, which can be time-consuming and resource-intensive.

Bayesian inference and MCMC methods provide a powerful framework to address these challenges in direct imaging. Rather than relying solely on algorithmic subtraction, modern postprocessing analyses construct generative residual models of the image, allowing the use of Bayesian hypothesis testing and MCMC sampling to infer contrasts, spectra, and uncertainties \citep{ruffioBayesianFrameworkExoplanet2018}. Using these methods high-priority exoplanets are determined for follow-up spectroscopic observations based on their inferred atmospheric properties \citep{bixelIdentifyingExoEarthCandidates2019}.

\subsection{Bayesian Classification of Non-Detections}

Direct imaging surveys currently produce non-detections roughly 98\% of the time \citep{ruffioBayesianFrameworkExoplanet2018}. These non-detections are determined by a defined upper limit to the signal-to-noise ratio (SNR), typically 5$\sigma$ \citep{meshkatFURTHEREVIDENCEPLANETARY2013,ruffioBayesianFrameworkExoplanet2018}. SNR is calculated by dividing the measured flux density at a given location by the standard deviation of the noise in an annulus around that location \citep{meshkatFURTHEREVIDENCEPLANETARY2013}:
\begin{equation}
  \text{SNR} = \frac{F_{\text{planet}}}{\sigma(r)\sqrt{\pi r_{\text{ap}}^2}}
\end{equation}
where $F_{\text{planet}}$ is the measured flux at the location of interest, $\sigma(r)$ is the standard deviation of the noise at radius $r$ from the star, and $r_{\text{ap}}$ is the radius of the photometric aperture used to measure the flux.

Image features below the $5\sigma$ limit are classified as non-detections. However, this method does not account for the possibility of false negatives, where a planet is present but undetected due to noise or other factors. To address this, \citet{ruffioBayesianFrameworkExoplanet2018} developed a Bayesian framework to classify non-detections, described below.

\subsubsection{Non-Detections at Known Locations}

In some cases, an exoplanet may be detected in a spectral band, but not be detected in another band, possibly due to overfiltering of the PSF or excessive noise in the image \citep{ruffioBayesianFrameworkExoplanet2018}. Bayesian inference can be used to determine a more probable upper limit on the planet brightness in the non-detection band.

Assuming the planet has a true flux density $F$, which is unknown. At its known location, we measure a flux $\tilde{F}$ with some uncertainty $\sigma$. The posterior $P(F|\tilde{F})$ is the probability distribution of the true flux $F$ given the measured flux $\tilde{F}$. Using Bayes' Theorem, \citet{ruffioBayesianFrameworkExoplanet2018} writes this as:
\begin{equation}
  P(F|\tilde{F}) = \frac{P(\tilde{F}|F)P(F)}{P(\tilde{F})}
\end{equation}

The prior $P(F)$ is the assumption that the flux is positive:
\begin{equation}
  P(F) = \begin{cases}
    \alpha & F > 0 \\
    0 & F \le 0
  \end{cases}
  \label{eq:flux_prior}
\end{equation}
where $\alpha$ is a constant depending on the range of $F$ values being considered.

Assuming Gaussian noise, the likelihood $P(\tilde{F}|F)$ is given by:
\begin{equation}
  P(\tilde{F}|F) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\tilde{F} - F)^2}{2\sigma^2}\right)
  \label{eq:gaussian_distribution}
\end{equation}
which is the probability of measuring flux density $\tilde{F}$ given a true flux density $F$ and uncertainty $\sigma$.

The evidence $P(\tilde{F})$ is a normalization constant, given by:
\begin{equation}
  P(\tilde{F}) = \int_{0}^{\infty} P(\tilde{F}|F)P(F) dF
  \label{eq:evidence}
\end{equation}

Using the posterior, the Bayesian upper limit $F_{\text{lim}}$ can be calculated depending on a desired confidence level, or \textit{cutoff probability} $\eta$, which is the probability that the true flux of the planet is below the upper limit \citep{ruffioBayesianFrameworkExoplanet2018}. This is given by:
\begin{equation}
  \eta = \int_{0}^{F_{\text{lim}}} P(F|\tilde{F}) dF
  \label{eq:upper_limit}
\end{equation}
Solving for $F_{\text{lim}}$ gives the Bayesian upper limit on the planet's flux at the desired confidence level. Conversely, using a predefined upper limit $F_{\text{lim}}$ will yield the corresponding cutoff probability $\eta$. This allows for $(a)$ a more accurate upper limit on the planet's brightness in the non-detection band, and $(b)$ a quantification of the probability of a false non-detection at 5$\sigma$.

\subsubsection{Non-Detections on Known Orbits}

The orbit of a theoretical exoplanet may be known from the existence of a gap in a protoplanetay disk \citep{ruaneDeepImagingSearch2017} or an other astrometric indicator. However, direct imaging may yield a non-detection at the expected location of the planet. Bayesian inference can be used to determine the probability that a planet is present on the known orbit.

In this case, the data $D$ is the entire image with a non-detection at the expected location of the planet. \citet{ruaneDeepImagingSearch2017} models this as a random vector:
\begin{equation}
  D = Fm(S) + N
\end{equation}
where $F$ is the unknown flux density of the planet, $m(S)$ is the PSF model at the expected location $S$ of the planet, and $N$ is the noise vector in the full image. Using Bayes' Theorem, the posterior $P(F|D)$ is given by:
\begin{equation}
  P(F|D) = \int \frac{P(D|F,S)P(F,S)}{P(D)} dS
\end{equation}
Note that the prior $P(F,S)$ includes both the flux density and location of the planet. It is reasonable to assume that the flux desntiy and location are independent, so that the posterior can be written as:
\begin{equation}
  P(F|D) = \int \frac{P(D|F,S)P(F)P(S)}{P(D)} dS
\end{equation}

The flux density prior $P(F)$ matches \autoref{eq:flux_prior}. The location prior $P(S)$, however, is based on the known orbit of the planet. The orbit defines the projected velocity of the planet $v_{\text{proj}}(S)$. Using this and the orbital period $T$, \citet{ruaneDeepImagingSearch2017} writes the location prior as:
\begin{equation}
  P(S) = \frac{1}{Tv_{\text{proj}}(S)}
\end{equation}
This prior gives higher probability to locations where the planet moves slowly, and lower probability to locations where the planet moves quickly.

Since the data are a random vector, \citet{ruaneDeepImagingSearch2017} expresses the likelihood $P(D|F,S)$ using a multivariate Gaussian distribution:
\begin{equation}
  P(D|F,S) = \frac{1}{\sqrt{2\pi | \Sigma |}} \exp\left(-\frac{1}{2}(D - Fm(S))^T \Sigma^{-1} (D - Fm(S))\right)
\end{equation}
This equation is the matrix form of \autoref{eq:gaussian_distribution}, where $\Sigma$ is the covariance matrix of the noise in the image, and $T$ denotes the transpose. Assuming the noise is independent of the data, the covariance matrix is diagonal.

The evidence $P(D)$ is a normalization constant, calculated using the same integration method as \autoref{eq:evidence}.

By integrating over all values of $S$, the Bayesian upper limit on the planet's flux $F_{\text{lim}}$ can be calculated at a desired cutoff probability $\eta$, as in \autoref{eq:upper_limit}.

\subsection{Bayesian Classification of Exo-Earths}

Direct imaging allows spectroscopic characterization of exoplanet atmospheres \citep{currieDirectImagingSpectroscopy2023}. This is particularly important for the detection of biosignatures, which are spectral features that indicate the presence of life. Common biosignatures include oxygen, ozone, methane, and water vapor \citep{seagerListMoleculesPotential2016}. The detection of these features in the atmosphere of an exoplanet could indicate the presence of life, or as evidence of habitability.

Two telescopes currently in development, the Habitable Exoplanet Observatory (HabEx) \textbf{[ADD FOOTNOTE]} and the Large UV/Optical/IR Surveyor (LUVOIR) \textbf{[ADD FOOTNOTE]}, aim to directly image Earth-like exoplanets, with an expected yield of dozens of exo-Earths. However, exo-Earth detection suffers from a false-positive rate of up to 77\% when using detection data from a single spectral band, and 47\% with prior constrains on the planet's orbit \citep{guimondDirectImagingSearch2018}. Furthermore, reducing the false-positive rate by collecting data from more bands is a time-consuming process of up to days or weeks per target \citep{bixelIdentifyingExoEarthCandidates2019}.

In order to minimize the telescope time required to confirm exo-Earth candidates (EECs), \citet{bixelIdentifyingExoEarthCandidates2019} developed a Monte Carlo framework to optimize the observing strategy. This framework uses Bayesian inference to classify EECs based on their observed spectra to prioritize targets for follow-up observations.
\begin{deluxetable}{lll}
  \tablewidth{0pt}
  \tablenum{3}
  \tablehead{
    \colhead{Case} & \colhead{Observational Data} & \colhead{Information Gained}
  }
  \startdata
    $1$ & Direct imaging detection in a single band & \\
    $2$ & + Radial velocity observations & Period and semi-amplitude \\
    $3$ & + Observations of a debris disk & Orbital inclination \\
    $4$ & + Multiple direct imaging epochs & Phase, semi-major axis, eccentricity \\
    $5$ & + Astromentry data & Mass \\
    $6$ & + Color information from multiple bands & Color-based discrimination\\
    $7$ & All of the above &
  \enddata
  \caption{The observables used to classify exo-Earth candidates. Each case includes additional information, with case 7 being the most comprehensive.}
  \label{tab:observables}
\end{deluxetable}

The framework compares two competing hypotheses: that the planet is an EEC, or that it is not. The comparison uses both the intrinsic properties $\theta$ (that determine if a planet is an EEC) and the observable quantities $x$ shown in \autoref{tab:observables}. They give this as a posterior probability $P(\theta|x)$ using Bayes' Theorem:
\begin{equation}
  P(\theta|x) = \frac{P(x|\theta)P(\theta)}{P(x)}
\end{equation}

To determine the prior $P(EEC)$, \citet{bixelIdentifyingExoEarthCandidates2019} constructed a Monte Carlo population of simulated planetary systems. They assigned each predicted planet a radius, mass, orbital period, eccentricity, and albedo drawn from the Kepler mission occurrence rates. From these properties, the corresponding observable quantities $x_{sim}$ were calculated, producing a prior sample of what future imaging surveys are likely to detect.

\citet{bixelIdentifyingExoEarthCandidates2019} calculated the likelihood $P(x|\theta)$ by comparing real observed quantities $x_{obs}$ to the simulated quantities $x_{sim}$ using a Gaussian distribution:
\begin{equation}
  P(x|\theta) = \prod_i \exp\left(\frac{(x_{obs,i} - x_{sim,i})^2}{2\sigma_{obs,i}^2}\right)
\end{equation}
where $\sigma_{obs,i}$ is the uncertainty in the observed quantity $x_{obs,i}$.

The evidence $P(x)$ is a normalization constant, calculated by integrating over all values of $\theta$:
\begin{equation}
  P(x) = \int P(x|\theta)P(\theta) d\theta
\end{equation}

The posterior probability $P(\theta|x)$ can then be used to classify the planet as an EEC or not. For each detected planet, the subset of simulated planets consistent with its observed brightness and separation (within measurement uncertainties) is identified, and the distribution of their physical properties defines the posterior. A threshold probability can be set, above which the planet is classified as an EEC, and below which it is classified as a non-EEC. This allows for the prioritization of targets for follow-up observations, reducing the telescope time required to confirm EECs.

Overall, \citet{bixelIdentifyingExoEarthCandidates2019} found that with single-band detections alone, even genuine exo-Earths cannot be identified with high confidence—typical posterior probabilities were below 50\%. However, adding incorporating more information about the exoplanet provides an additional powerful discriminant between Earth-like and non-habitable planets. In Case 7 (the most data-rich scenario), the average confidence in correctly identifying exo-Earths exceeded 80–85\%. This is equivalent to reducing the false-positive rate to below 20\%, a significant improvement over direct imaging alone.

\section{CMB Parameter Estimation}
\label{sec:CaseStudies_CMB}

\subsection{Introduction}

The Cosmic Microwave Background (CMB) is the residual thermal radiation from the Big Bang, permeating the entire universe. It provides a snapshot of the universe when it was just 380,000 years old, offering invaluable insights into its early conditions, composition, and evolution \citep{staggsRecentDiscoveriesCosmic2018}. The CMB is remarkably uniform, with temperature fluctuations at the level of one part in 100,000. These tiny anisotropies, discovered by \citet{smootStructureCOBEDifferential1992}, encode information about the density fluctuations that eventually led to the formation of galaxies and large-scale structures in the universe \citep{planckcollaborationPlanck2018Results2020}. Such structures have been attributed to the gravitational collapse of primordial overdensities, which are believed to have originated from quantum fluctuations during the inflationary epoch \citep{guthInflationaryUniversePossible1981}. These overdensities are described by a power spectrum $P(k)$, which quantifies the amplitude of fluctuations as a function of spatial scale (wavenumber $k$) \citep{staggsRecentDiscoveriesCosmic2018}.

The analysis of CMB data has been instrumental in establishing the standard model of cosmology, known as the Lambda Cold Dark Matter ($\Lambda$CDM) model. This model posits that the universe is composed of approximately 5\% ordinary matter, 27\% dark matter, and 68\% dark energy \citep{planckcollaborationPlanck2015Results2016}. $\Lambda$CDM is based on six fundamental parameters: the physical densities of baryons ($\Omega_b h^2$) and cold dark matter ($\Omega_c h^2$), the angular size of the sound horizon at recombination ($\theta_s$), the optical depth due to reionization ($\tau$), the amplitude of the primordial power spectrum ($A_s$), and the scalar spectral index ($n_s$) \citep{planckcollaborationPlanck2018Results2020}. The final Planck analysis provides the most precise measurements of these parameters to date, with uncertainties at the sub-percent level for most parameters \citep{planckcollaborationPlanck2018Results2020}. The values are shown in \autoref{tab:planck_params}.
\begin{deluxetable}{lll}
  \tablewidth{0pt}
  \tablenum{4}
  \tablehead{
    \colhead{Parameter} & \colhead{Value} & \colhead{Description}
  }
  \startdata
    $\Omega_b h^2$ & $0.0224 \pm 0.0001$ & Physical baryon density \\
    $\Omega_c h^2$ & $0.120 \pm 0.001$ & Physical cold dark matter density \\
    $\theta_s$ & $0.01041 \pm 0.00001$ & Angular size of sound horizon \\
    $\tau$ & $0.054 \pm 0.007$ & Optical depth to reionization \\
    $\ln(10^{10} A_s)$ & $3.044 \pm 0.014$ & Amplitude of primordial power spectrum \\
    $n_s$ & $0.9649 \pm 0.0042$ & Scalar spectral index \\
  \enddata
  \caption{The six parameters of the $\Lambda$CDM model as measured by the Planck satellite \citep{planckcollaborationPlanck2018Results2020}.}
  \label{tab:planck_params}
\end{deluxetable}

\subsection{Challenges}

Despite the remarkable success of CMB observations in constraining the six-parameter $\Lambda$CDM model, extracting cosmological parameters with greater precision from CMB data confronts several substantial challenges that complicate the analysis.

The most fundamental challenge arises from geometric and acoustic degeneracies among cosmological parameters. Since the CMB power spectrum encodes information about how sound waves propagate through the early universe, different combinations of parameters can produce nearly indistinguishable anisotropy patterns \citep{efstathiouCosmicConfusionDegeneracies1999}. The primary geometric degeneracy relates the angular size of the sound horizon at recombination $r_s$ to the comoving anglular-diameter distance $D_A(z_*)$ through combinations of total matter density $\Omega_m$, dark energy density $\Omega_\Lambda$, and the Hubble parameter $H_0$ \citep{efstathiouCosmicConfusionDegeneracies1999}, where
\begin{equation*}
  \Omega_m = \frac{\Omega_b h^2 + \Omega_c h^2}{h^2} \qquad \text{and} \qquad \Omega_\Lambda = 1 - \Omega_m - \Omega_k
\end{equation*}
where $\Omega_k$ is the curvature density, which is consistent with zero based on CMB observations \citep{planckcollaborationPlanck2018Results2020}. This means that increasing $\Omega_\Lambda$ while simultaneously reducing $\Omega_m$ in the right proportion keeps the position of acoustic peaks nearly unchanged, producing a strong correlation between these parameters.

Furthermore, extracting the six $\Lambda$CDM parameters from CMB data requires sampling from a high-dimensional posterior probability distribution in the presence of strong parameter correlations. MCMC methods are the standard approach, but they face significant computational challenges when the number of dimensions exceeds six or seven \citep{wandeltStatisticalChallengesCosmic2003}. This is beacause the likelihood function for CMB data is extremely complex, with each likelihood evaluation requiring running a relativistic Boltzmann solver (such as CAMB or CLASS) to generate predicted CMB power spectra as a function of proposed parameters \citep{wandeltStatisticalChallengesCosmic2003,aricoAcceleratingLargeScaleStructureData2022}. For high-precision analyses, millions of likelihood evaluations may be required to adequately sample the posterior, leading to total computation times of weeks or months on high-performance computing clusters.

CMB cosmology stands at the frontier of precision: the challenges of parameter degeneracies and computational complexity must be overcome to extract maximal science return from exquisite measurements. Future missions such as CMB-S4 \textbf{[ADD FOOTNOTE]}, LiteBIRD\textbf{[ADD FOOTNOTE]}, and Simons Observatory\textbf{[ADD FOOTNOTE]} will push sensitivity and resolution further, demanding ever-more sophisticated data analysis strategies and highlighting the necessity of MCMC and Bayesian methods for robustly inferring fundamental cosmological parameters \citep{dunkleyFastReliableMarkov2005}. In order to meet these challenges, improvements to MCMC algorithms have been proposed to reduce the computational cost of building likelihoods with Boltzmann solvers \citep{akeretCosmoHammerCosmologicalParameter2013}. On the other hand, novel methods such as Bayesian Neural Networks (BNNs) respresent a divergence from traditional MCMC appriaches \citep{hortuaParameterEstimationCosmic2020}.

\subsection{Optimizations to MCMC}

Traditionally, CMB parameter estimation has relied on MCMC methods such as the Metropolis-Hastings algorithm. \citet{lewisCosmologicalParametersCMB2002} developed CosmoMC, a widely used MCMC engine specifically designed for cosmological parameter estimation from CMB data, based on the Metropolis-Hastings algorithm and the Boltzmann solver CAMB \citep{lewisEfficientComputationCosmic2000}. While CosmoMC has been highly successful, it faces significant computational challenges due to the high dimensionality of the parameter space and the complexity of the likelihood function \citep{akeretCosmoHammerCosmologicalParameter2013}. Metropolis-based MCMC methods reqire the evaluation of the likelihood function at each proposed step, which involves running a Boltzmann solver to generate predicted CMB power spectra. This can be computationally expensive, especially when millions of likelihood evaluations are needed to adequately sample the posterior distribution \citep{wandeltStatisticalChallengesCosmic2003}.

To address these challenges, \citet{akeretCosmoHammerCosmologicalParameter2013} developed CosmoHammer, an MCMC framework that leverages parallel computing to accelerate the sampling process. CosmoHammer uses the affine-invariant ensemble sampler implemented in the \texttt{emcee} package \citep{foreman-mackeyEmceeMCMCHammer2013}, which is well-suited for high-dimensional parameter spaces with strong correlations. The methodology of CosmoHammer is covered in this section.

Given a prior $q(\theta)$ and a likelihood function $\mathcal{L}(\theta)$, the posterior distribution is given by Bayes' Theorem:
\begin{equation}
  P(\theta) \propto \mathcal{L}(\theta) q(\theta)
\end{equation}
In this case, $\theta$ represents the set of cosmological parameters being estimated. The evidence has been omitted, as it is a normalization constant that does not affect the sampling process.

$\mathcal{L}(\theta)$ depends on the CMB power spectra, which can only be evaluated numerically via a Boltzmann solver such as CAMB, which has alread been established to be computationally expensive at large scales. To mitigate this, a parellelized approach is used based on the GW algorithm \citep{goodmanEnsembleSamplersAffine2010}, where multiple chains (or \textit{walkers}) are run simultaneously on different processors, and the \texttt{emcee} package \citep{foreman-mackeyEmceeMCMCHammer2013}, which further optimizes likelihood evaluations by dividing walkers into two groups that update their positions based on the other group's positions. This allows for efficient exploration of the parameter space while minimizing the number of Boltzmann solvers required.

CosmoHammer wraps the \texttt{emcee} sampler and CAMB Boltzmann solver into a modular framework that can be easily deployed on high-performance computing clusters. The framework allows for the parallel execution of multiple walkers, with each walker evaluating the likelihood function independently on a separate processor. This parallelization significantly reduces the overall computation time required to sample the posterior distribution. Furthermore, CosmoHammer includes features such as adaptive step sizes and convergence diagnostics to ensure efficient sampling and reliable parameter estimates.

CosmoHammer achieves comparable statistical precision to CosmoMC for while dramatically reducing wall-clock time by exploiting ensemble parallelism. With 350 walkers and 250 post-burn-in steps (87,500 samples), CosmoHammer attains mean errors around 3.4\% of each parameter's marginal standard deviation. CosmoMC achieves similar precision with 35,000 post-burn-in samples, but it pays substantial tuning overhead if not pre-tuned, requiring on the order of 2000 adaptation steps before efficient sampling begins. In wall time, analysis that takes ~30 hours on a dual-core laptop with a single long Metropolis-Hastings chain can be completed in ~16 minutes using CosmoHammer on 2048 cores.

\subsection{Bayesian Neural Networks}

Alternatives to MCMC methods involving neural networks have also been proposed for CMB parameter estimation. A typical neural network is a graph of interconnected nodes or \textit{neurons} organized into layers. Each neuron applies a nonlinear activation function to a weighted sum of its inputs, allowing the network to learn complex mappings from inputs to outputs \citep{gurneyIntroductionNeuralNetworks2018}. Neural networks have been used extensively in cosmology for tasks such as model discrimination \citep{schmelzleCosmologicalModelDiscrimination2017} and cosmic string detection \citep{ciucaConvolutionalNeuralNetwork2019}. However, traditional neural networks provide point estimates of parameters without quantifying uncertainties, and they can be prone to overfitting, especially when trained on limited data \citep{hortuaParameterEstimationCosmic2020}. The most promising solution to these issues is the use of Bayesian Neural Networks (BNNs) for parameter estimation \citep{galDropoutBayesianApproximation2016}. In a BNN, the weights of the network are treated as random variables with prior distributions, allowing for uncertainty quantification in the predictions \citep{hortuaParameterEstimationCosmic2020}. The BNN approach to CMB parameter estimation developed by \citet{hortuaParameterEstimationCosmic2020} is covered in this section.

Given the prior distribution over weights $p(w)$ and the likelihood function $p(D|w)$, the posterior distribution over weights given the data $D$ is given by Bayes' Theorem:
\begin{equation}
  p(w|D) = \frac{p(D|w) p(w)}{p(D)}
\end{equation}
where $P(D)$ is the evidence, which is a normalization constant. If the data consists of input-output pairs $(x,y)$, the likelihood function can be expressed as:
\begin{equation}
  p(D|w) = \prod_{i} p(y_i|x_i, w)
\end{equation}
where $p(y_i|x_i, w)$ is the probability of observing output $y_i$ given input $x_i$ and weights $w$.

However, computing the posterior distribution over weights is intractable for most neural networks due to the high dimensionality of the weight space. To address this, \citet{hortuaParameterEstimationCosmic2020} employ variational inference, which approximates the true posterior $p(w|D)$ with a simpler distribution $q(w|\theta)$ parameterized by $\theta$. The parameters $\theta$ are optimized by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximate distribution:
\begin{equation}
  \text{KL}(q(w|\theta) || p(w|D)) = \int q(w|\theta) \ln \frac{q(w|\theta)}{p(w|D)} dw
\end{equation}

Once the posterior distribution over weights is obtained, predictions for new inputs $x^*$ can be estimated by marginalizing over the weights:
\begin{equation}
  p(y^*|x^*, D) = \int p(y^*|x^*, w) p(w|D) dw
\end{equation}
The posterior predictive distribution $p(y^*|x^*, D)$ can be estimated by Monte Carlo sampling. This allows the BNN to produce both a mean prediction and credible intervals that reflect epistemic (model-based) and aleatoric (data-based) uncertainties.

To train the networks, 50,000 synthetic full-sky CMB realizations were generated using the Boltzmann solver CLASS and HEALPY. Each realization was projected into $20\times20$ degree patches of $256\times256$ pixels. The model varied three $\Lambda$CDM parameters: baryon density $\Omega_b h^2$, cold dark matter density $\Omega_c h^2$, and the amplitude of the primordial power spectrum $A_s$. Other parameters were fixed to values from \citet{planckcollaborationPlanck2018Results2020}.

BNNs were able to recover the posterior distributions of cosmological parameters with correlations comparable to MCMC methods. Using the best-performing configuration, the network generated thousands of posterior samples in seconds, achieving speedups to the order of $10^4$ over MCMC while maintaining realistic uncertainty estimates.

\section{Other Case Studies}
\label{sec:CaseStudies_Other}

Beyond the applications covered in \hyperref[sec:CaseStudies_DirectImaging]{Section III} and \hyperref[sec:CaseStudies_CMB]{Section IV}, MCMC methods have been successfully applied to a variety of other astrophysical parameter estimation problems. A non-exhaustive list of such applications includes work in Gravitational Wave paremeter estimation, Dark Matter detection, and Black Hole modeling.

\subsection{Gravitational Wave Parameter Estimation}

Gravitational waves (GWs) are ripples in spacetime caused by accelerating massive objects, such as merging black holes or neutron stars, first detected by \citep{abbottObservationGravitationalWaves2016}. The detection and analysis of GWs provide a new window into the universe, allowing for the study of extreme astrophysical phenomena and tests of general relativity \citep{abbottObservationGravitationalWaves2016}. MCMC methods have been extensively used in GW data analysis to estimate the parameters of GW sources from noisy detector data. These parameters include the masses, spins, sky locations, and distances of the merging objects \citep{christensenParameterEstimationGravitational2022}. The likelihood function for GW data is typically complex, involving the convolution of theoretical waveform models with detector noise characteristics \citep{veitchParameterEstimationCompact2015}. MCMC algorithms, such as the Metropolis-Hastings and Hamiltonian Monte Carlo, have been employed to sample the posterior distributions of source parameters, including masses, spins, and sky locations \citep{christensenParameterEstimationGravitational2022,veitchParameterEstimationCompact2015}.

The key challenge MCMC methods face is the high dimensionality of the parameter space, often exceeding 15 dimensions \citep{christensenParameterEstimationGravitational2022}. Like CMB parameter estimation (see \hyperref[sec:CaseStudies_CMB]{Section IV}), traditional MCMC methods can struggle to efficiently explore such high-dimensional spaces \citep{veitchParameterEstimationCompact2015}. This complexity requires efficient sampling techniques and careful tuning of proposal distributions to ensure convergence and adequate exploration of the posterior.

These challenges have been addressed by optimizing MCMC algorithms for GW data analysis. For example, \citet{wongFastGravitationalWave2023} developed a high-performance Bayesian inference framework by integrating likelihood heterodyning and a gradient-based MCMC sampler to optimize parameter estimation. Likelihood heterodyning reduces the computational cost of evaluating the likelihood function by taking advantage of the fact that waveforms corresponding to nearby points in parameter space are very similar, meaning that their ratio varies smoothly with frequency. By decomposing the likelihood into a fixed, highly oscillatory reference term and a smooth ratio term, the likelihood can be computed accurately using far fewer frequency bins \citep{zackayRelativeBinningFast2018}. This dramatically accelerates likelihood evaluations without loss of precision. The framework also employs a gradient-based MCMC sampler: the Metropolis-adjusted Langevin algorithm (MALA) \citep{grenanderRepresentationsKnowledgeComplex1994}.  MALA uses the gradient of the posterior probability to propose new samples more efficiently. Instead of exploring parameter space through random jumps, it moves preferentially toward regions of higher posterior probability.

In injection-recovery tests of 1,200 synthetic GW signals, the \citet{wongFastGravitationalWave2023} pipeline successfully recovered the injected parameters with statistically consistent credible intervals. When applied to real events, the method produced posterior distributions consistent with previous estimations, but with speedup to the order of $10^3$.

\subsection{Dark Matter Detection}

Dark matter comprises approximately 85\% of the matter in the universe, yet its nature remains one of the most fundamental unsolved problems in physics \citep{cushmanWIMPDarkMatter2013}. Direct detection experiments aim to observe the scattering of dark matter particles, such as weakly interacting massive particles (WIMPs), off atomic nuclei in deep underground detectors such as XENON and LUX \citep{arinaBayesianAnalysisMultiple2014}. Parameter estimation in these experiments involves constraining dark matter particle model parameters such as the WIMP-nucleon scattering cross section, dark matter mass, and interaction type using nuclear recoil energy spectra and event rates \citep{cushmanWIMPDarkMatter2013}.

The primary challenge in dark matter direct detection analysis, as with many other fields, is the computational expense of likelihood function evaluation combined with the high dimensionality of particle physics parameter spaces \citep{cerdenoBayesianTechniqueCombine2025}. To address these challenges, \citet{cerdenoBayesianTechniqueCombine2025} developed  the Truncated Marginal Neural Ratio Estimation (TMNRE) approach. TMNRE avoids explicit likelihood calculations by training neural networks on simulated dark matter scattering data to estimate the likelihood ratio from the data alone, similar to the BNN approach discussed in \hyperref[sec:CaseStudies_CMB]{Section IV}. In validation studies comparing TMNRE with conventional MCMC for WIMP dark matter with spin-dependent and spin-independent interactions with protons and neutrons in xenon experiments, the machine learning approach recovered identical posterior distributions to traditional MCMC but achieved speedups of several orders of magnitude.

\subsection{Black Hole Modeling}

Black holes are among the most extreme objects in the universe, and measuring their fundamental properties provides crucial tests of general relativity and constraints on black hole formation and evolution mechanisms. X-ray reflection spectroscopy has emerged as the leading method for measuring black hole spins in both stellar-mass black holes in X-ray binaries and supermassive black holes in active galactic nuclei (AGN) \citep{reynoldsMonteCarloMarkov2012}. This technique exploits relativistically broadened iron emission lines from black hole accretion disks, where frame-dragging effects depend sensitively on black hole spin \citep{reynoldsMonteCarloMarkov2012}.

Black hole spin measurement via X-ray reflection spectroscopy faces significant challenges from degeneracies between model parameters \citep{reynoldsMonteCarloMarkov2012}. The analysis requires fitting complex spectral models with numerous parameters including black hole spin, accretion disk inclination angle, iron abundance, reflection fraction, and coronal geometry. A critical degeneracy exists between the iron abundance of the innermost accretion disk and the black hole spin parameter: supersolar iron abundances can mimic the spectral signatures of rapid spin, or alternatively, rapid spin can appear less extreme when paired with subsolar iron abundance assumptions.

\citet{reynoldsMonteCarloMarkov2012} employed MCMC-based spectral analysis to address this degeneracy through analysis of high-quality X-ray spectra from the Suzaku satellite for the AGN NGC 3783. The MCMC analysis revealed a strong correlation between iron abundance and black hole spin, demonstrating that assuming solar iron abundance led to biased spin estimates. By allowing iron abundance to vary freely in the MCMC fitting, they obtained a more accurate and reliable measurement of the black hole spin parameter. The analysis concluded that NGC 3783 hosts a rapidly spinning black hole with spin parameter $a > 0.88$, while also constraining the iron abundance to be supersolar at $Z_{\text{Fe}} = 2-4 Z_{\odot}$.

\citet{sisk-reynesEvidenceModerateSpin2022} further applied MCMC-based X-ray reflection spectroscopy to measure the spin of the supermassive black hole in the quasar H1821+643 using data from the Chandra X-ray Observatory. Their MCMC analysis yielded a the first well-defined spin measurement of $a = 0.62^{+0.22}_{-0.37}$, an inclination angle of $i = 44.6^{+3.3}_{-3.3}$, and an iron abundance of $Z_{\text{Fe}} = 1.02^{+1.33}_{-0.38} Z_{\odot}$.

\subsection{Further Case Studies}

Many more applications of Bayesian inference and MCMC methods exist in astrophysics beyond those covered in this review. A few notable examples are listed, along with their references for further reading:
\begin{enumerate}
  \item Creation of pulsar timing arrays \citep{laiAcceleratedBayesianInference2025}
  \item Quasar light curve analysis \citep{donnanBayesianAnalysisQuasar2021}
  \item Asteroid mass estimation \citep{siltalaAsteroidMassEstimation2020}
\end{enumerate}

Overall, MCMC methods have become indispensable tools across a wide range of astrophysical problems, enabling robust inference in the presence of complex models, high-dimensional parameter spaces, and challenging data characteristics. Despite the computational challenges involved, ongoing methodological advancements continue to expand the applicability and efficiency of MCMC techniques in astrophysics for application to further research.

\section{Discussion}

\subsection{Optimization versus Transformation}

The case studies reveal a fundamental pattern: Bayesian-MCMC frameworks are primarily being leveraged to optimize and refine existing research methodologies rather than to fundamentally transform the underlying scientific questions being asked.

In the direct imaging of exoplanets (\hyperref[sec:CaseStudies_DirectImaging]{Section III}), Bayesian inference is applied to enhance the rigor of detection thresholds and to prioritize targets for follow-up observations. Rather than replacing traditional PSF subtraction techniques, Bayesian methods augment these approaches by providing uncertainty quantification and hypothesis testing frameworks. Similarly, in CMB parameter analysis (\hyperref[sec:CaseStudies_CMB]{Section IV}), the introduction of CosmoHammer and ensemble MCMC methods does not change the core physics of parameter estimation. The approaches serve to accelerate computation while remaining faithful to the underlying Boltzmann solvers. \hyperref[sec:CaseStudies_Other]{Section V} further illustrates this pattern: sophisticated MCMC implementations and advanced likelihood engineering reduce computational barriers but preserve the essential physical models and methodologies.

This observation reflects the maturity of the field. In domains where physical theories are well-developed and observational techniques are proven, the role of statistical methods is to extract maximum precision from existing frameworks. The case studies demonstrate that modern astrophysics has largely solved the methodological question of how to do Bayesian inference at scale; the remaining focus is on doing it faster and more efficiently.

As of the time of writing, MCMC methods such as Metropolis-Hastings variants and ensemble samplers remain the standard for Bayesian parameter estimation across astrophysics. This dominance is well-earned: MCMC methods are widely understood and validated against decades of astronomical data. Their robustness in handling complex likelihood functions, multi-modal posteriors, and high-dimensional parameter spaces makes them a reliable tool. Furthermre, tuning MCMC for a well-understood problem is a lower-risk path than adopting untested methods. Researchers can leverage existing codebases, validation protocols, and community expertise. The payoff is immediate and quantifiable—wall-clock time reduced by orders of magnitude translates directly to increased scientific productivity.

\subsection{Competing Paradigms}

Beneath the optimization narrative, a competing methodological paradigm is emerging. The case studies containing Bayesian Neural Networks (Section 4.4) and neural ratio estimation (Section 5.2) represent a departure from traditional MCMC. These approaches do not merely accelerate the existing framework, but replace it with a learned emulator or surrogate model, fundamentally altering how statistical inference is performed.

The results are striking. \citet{hortuaParameterEstimationCosmic2020} achieved 10,000-fold speedups for CMB parameter estimation while maintaining realistic uncertainty quantification. \citet{cerdenoBayesianTechniqueCombine2025} demonstrated that neural ratio estimation recovers identical posterior distributions to MCMC but at speedups of several orders of magnitude. These are far from incremental gains. Instead, Bayesian Neural Networks represent a different regime of computational feasibility.

Yet, these methods have yet to reach a point of general use in astrophysics. The majority of active research continues to refine MCMC pipelines rather than exploring neural network alternatives. This lag may be a result of both institutional inertia and technical questions about the reliability of machine learning methods for novel problems. \citet{hortuaParameterEstimationCosmic2020} outlines a number of concerns about the application of deep learning models in physics, including the models being prone to overfitting and generating unreliable data. Though \citet{hortuaParameterEstimationCosmic2020} studied a Bayesian treatment of these models as a possible solution to these concerns, the BNNs still required large training datasets of simulated data. When confronted with observational regimes that differ substantially from the training domain, the question of these method's applicability remains.

The existence of these competing methodologies, however, creates a decision point for practitioners: when is it scientifically and computationally justified to adopt advanced machine learning approaches over optimized MCMC?

Based on the case studies, MCMC tended to be the optimal choice for when:
\begin{enumerate}
  \item The posterior was well-understood:\\When prior physics strongly constrained the parameter space (as in CMB cosmology with the six $\Lambda$CDM parameters), MCMC could be tuned to exploit this structure. The dimensionality remained manageable, and validation was straightforward.
  \item Likelihood evaluations were not expensive:\\If the bottleneck was not likelihood computation but rather posterior exploration, MCMC techniques provided substantial gains without the overhead of training machine learning models.
  \item Interpretability and explainability were critical:\\When a clear understanding was required of why particular parameters were inferred, MCMC was the better chouce due to its transparent sampling history. Neural network predictions, on the other hand, acted as a black box.
\end{enumerate}

On the contrary, machine learning approached were used when likelihood evaluation was the computational bottleneck. When thousands or millions of likelihood evaluations were required, amortizing the cost through pre-trained emulators became optimal. CMB parameter estimation is an example of this: thousands of Boltzmann solver calls justify the upfront investment in training a neural network.

Looking forward, the most likely scenario is not the wholesale replacement of MCMC but rather methodological differentiation based on problem characteristics. Established, well-understood research programs with stable parameter spaces will increasingly adopt neural network-based methods, trading some interpretability for computational efficiency. Frontier research, exploratory studies, and problem domains with evolving theoretical frameworks will retain MCMC as the preferred tool for its transparency and generality.

The field will ultimately benefit from both traditions existing in productive tension. MCMC methods will continue to improve through algorithmic refinement and better proposal engineering. Machine learning approaches will mature through larger training datasets and improved uncertainty quantification. The division of labor will reflect the nature of the science: MCMC for precision measurement and validation; neural networks for throughput and scale.

\section{Conclusion}

Bayesian inference and MCMC methods have become essential infrastructure for contemporary astrophysics. However, these methods are optimizing the existing scientific basis rather than fundamentally transforming it. Whether applied to refining PSF subtraction in direct imaging or accelerating Boltzmann solver calls in cosmology, the statistical machinery operates within well-established theoretical frameworks. The advancement manifests itself by extracting more precision and speed from known physics.

This maturation reflects the field's sophistication. Modern astrophysics has largely resolved the technical question of how to perform Bayesian inference at the scales demanded by contemporary observations. The remaining challenge is methodological: choosing among competing approaches that increasingly offer comparable accuracy with vastly different computational profiles.

The case studies reveal clear patterns in this choice. MCMC retains dominance in established, well-understood research programs where interpretability and generality are valued, and where likelihood evaluations, while expensive, are not the primary bottleneck. Machine learning alternatives emerge as compelling when amortizing computational cost becomes advantageous—in high-dimensional problems. The question is not which method is universally superior, but rather which method best matches the scientific problem at hand.

\subsection{Future Work}

Despite the maturity of MCMC and the promise of machine learning alternatives, the case studies reveal several gaps that neither paradigm has fully addressed.

Firstly, Neither MCMC nor BNNs naturally address scenarios where the physical model itself may be incomplete or incorrect. Both methods assume the model is true and focus on inferring parameters. Developing Bayesian frameworks that simultaneously infer parameters and model uncertainty remains an open frontier.

Furthermore, as surveys like LSST and Rubin Observatory generate unprecedented data volumes, the question becomes not just how to estimate parameters for individual sources but how to allocate computational resources across millions of sources, some of which may be faint or unusual. This problem of combining statistical efficiency with practical resource management has not been comprehensively addressed by either MCMC or neural network approaches.

Bayesian inference through MCMC has established itself as a cornerstone methodology in astrophysics: reliable across diverse applications, adaptable to novel problems, and increasingly augmented by machine learning alternatives. While these methods will not fundamentally alter the underlying physics being studied, they have transformed how that physics is evaluated. MCMC provides a principled computational framework that converts complex observational data into quantitative constraints on physical models, enabling discoveries that observations alone cannot reveal.

Future astrophysics will demand both the interpretability and robustness of MCMC and the scalability and efficiency of machine learning approaches. The most productive path forward lies in recognizing these as complementary rather than competing paradigm. As astronomical surveys and physical simulations grow increasingly sophisticated, the ability to fluently deploy multiple inference strategies will distinguish the most effective research programs.

\appendix
\section[Appendix A]{Radial Velocity Brute-Force Approach}
\label{appx:A}

\lstinputlisting[style=pythonstyle]{../scripts/2.2/brute-force-ex.py}

\section[Appendix B]{Radial Velocity MCMC Approach}
\label{appx:B}

\lstinputlisting[style=pythonstyle]{../scripts/2.5/mcmc-ex.py}

\bibliography{references}

\end{document}
