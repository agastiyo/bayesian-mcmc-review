\documentclass[preprint,longauthor]{aastex631}

\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{etoolbox}

\makeatletter
\pretocmd{\@footnotetext}{\hbadness=10000 \hfuzz=1pt}{}{}
\makeatother

\numberwithin{equation}{section}

\lstdefinestyle{pythonstyle}{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{teal!70!black}\itshape,
  stringstyle=\color{orange!85!black},
  numberstyle=\tiny\color{gray},
  numbers=left,
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!5},
  frame=tb,
  rulecolor=\color{black!30},
  breaklines=true,
  showstringspaces=false,
  tabsize=2,
  captionpos=b
}

% Note to self: Don't forget to start up Zotero when you start writing!

\begin{document}
\label{placeholder} % Delete this when done!

\title{Bayesian Inference and MCMC Methods in Astrophysics}
\author{Agastya Gaur}
\affiliation{University of Illinois at Urbana-Champaign}

\begin{abstract}
  \blindtext
\end{abstract}

\keywords{Astrophysics, Astrostatistics, Bayesian Statistics, Markov Chain Monte Carlo, Big Data}

\section{Introduction}
\label{sec:Introduction}
\subsection{The History of Astrostatistics}
In the 4th century BC, Hipparchus, attempting to estimate the length of a year, found the middle of the range of a scattered set of Babylonian solstice measurements \citep{feigelsonStatisticalChallengesModern2004}. An achievement for the time, Hipparchus's measurement marked the beginning of what would become a long-standing marriage between astronomy and statistics. In the centuries to come, a number of breakthroughs in astrostatistics followed. Notably, Tycho Brahe in the late 1500s made repeated positional measurements of stars using naked-eye observations. The data were so precise that their modeling led to Kepler's new laws of planetary motion \citep{leavesleyTychoBrahesWay2018}. Furthermore, in the 1770s, Laplace rediscovered Bayesian statistics, and over the next decade he expanded upon the theory, using it to modernize Newton's theory of gravity \citep{stiglerStudiesHistoryProbability1975}.

The biggest advance in astrostatistics before the era of computing was in 1805 when Legendre published the method of least squares regression to model the orbit of comets \citep{feigelsonStatisticalChallengesModern2004}. He theorized that the model best fit to a set of data was one that minimized the sum of the squares of the errors. Though Legendre did not provide a formal proof of the method, regarding it only as a convenient trick, later works by Robert Adrain developed formal mathematical proofs of the method \citep{merrimanHistoryMethodLeast1877}. In 1809, Gauss published his own work on least squares, using it to calculate the orbit of the dwarf planet Ceres. Controversially, he also insisted that he had discovered the method years before Legendre \citep{stiglerGaussInventionLeast1981}. Through its impact, least squares regression has cemented itself in history as one of the most important leaps in astrostatistics.

The recurring theme is clear: progress in astronomy often hinges on solving problems of statistical estimation. By the end of the century, astronomy had firmly established itself as a quantitative science, driven by the refinement of statistical methods to identify regularities in scattered measurements, fitting orbital models, and quantifying uncertainty in the presence of noise.

\subsection{The Impact of Astrophysics}

Later years brought two developments that reshaped the relationship between astronomy and statistics: the rise of physics as the foundation of astronomy and the advent of computing, which enabled unprecedented scales of data analysis. As astronomy grew increasingly intertwined with the theories of physics, the field transformed into what we now call astrophysics. Though a niche field called statistical astronomy persisted, the majority of astronomers made insufficient use of statistics in their work \citep{feigelsonStatisticalChallengesModern2004}. The focus shifted to deriving physical models from first principles, and statistical methods were often seen as secondary or even unnecessary. \citet{hubbleDistributionLuminosityElliptical1930} determined the fit for the light curve of elliptical galaxies by trial-and-error instead of regression. \citet{zwickyMassesNebulaeClusters1937} first observed dark matter using a curve fitted only by eye.

The disdain for statistics stemmed from most astrophysicists' stubborn adherence to Newtonian determinism. Physics was regarded as the fundamental law of nature and an elegant basis for astronomy, while statistics was seen as rough, approximate, and imperfect. Statistics also flourished in the social sciences, further alienating it from astrophysics.

However, statistics would not be kept away from astronomy for long. As computing machines developed, astronomers increasingly adopted new tools for both calculation and simulation. In the 1920s, one of the earliest applications appeared in the production of lunar tables. Previously, astronomers calculated the position of the Moon using complex, error-prone methods that required extensive manual computation \citep{duncombeEarlyApplicationsComputer1988}. By the 1930s, however, \citet{comrieApplicationHollerithTabulating1932} demonstrated how punch card computing machines could automate the process, making lunar ephemerides faster and more reliable. From then on to the 1970s, Comrie's work was continued by Wallace Eckert, who improved the punch-card calculations using IBM computers \citep{olleyTaskThatExceeded2018}. Like lunar calculations, galactic simulations were also performed with computational devices as early as the 1940s. \citet{holmbergClusteringTendenciesNebulae1940} modeled gravitational interactions using lightbulbs to represent galaxies, demonstrating how spiral structures could emerge. These analog demonstrations laid the groundwork for digital computer N-body simulations in the 1970s, such as \citet{toomreGalacticBridgesTails1972}, who explained tidal tails and bridges in interacting galaxies.

With these advances in computing came a natural resurgence of statistical methods in astronomy. Given the ability to automate calculations, handle larger datasets, and simulate complex systems, statistical analysis became indispensable. In the mid-20th century, the growth of galaxy surveys encouraged quantitative modeling of structure and dynamics. Early work such as \citet{lynden-bellStatisticalMechanicsViolent1967} applied statistical mechanics to stellar systems, laying the foundation for the study of galaxy formation and equilibrium. By the 1970s, statistics was increasingly recognized as a distinct methodological pillar of astronomy. \citet{peeblesStatisticalAnalysisCatalogs1973} systematically catlogued and analyzed extragalactic objects using power spectra and correlation functions, pioneering the statistical study of large-scale structure.

While the early history of the field was dominated by statistical reasoning and theory, the growth of physics and digital computation broadened this into what we now call quantitative analysis (QA). Quantitative analysis thus represents the merging of three traditions that once stood apart: the deductive rigor of physics, the inferential power of statistics, and the scalability of computation. In modern astronomy, progress often relies not on one of these strands in isolation but on their integration. QA therefore serves as both a methodological framework and a philosophy of practice, emphasizing reproducibility, uncertainty quantification, and the ability to extract physical meaning from complex data.

\subsection{The Data Deluge}

Today, astrophysics sits in a universe of complex statistical problems that demand new quantitative approaches and more computing power with each passing day. These are compounded by an unprecedented era of astronomical data generation in the 21st century. Sky surveys like Gaia DR3 alone provide astrometry and photometry for nearly two billion stars, plus more than ten million variable sources \citep{gaiacollaborationGaiaDataRelease2023}. The nineteenth data release of the Sloan Digital Sky Survey collected spectroscopic data from over 6 million objects \citep{sdsscollaborationNineteenthDataRelease2025}. Advances in CCD detectors will see data from sky surveys increase in the next decade from gigabytes to terabytes today, and possibly to petabytes in the near future. The same trend can be seen in data from NASA's Solar Dynamics Observatory, which now generates over a terabyte of data per day, and the Rubin LSST, generating close to 30 terabytes per day \citep{borneAstroinformatics21stCentury2009}. Compared to the Henry Draper Catalogue \citep{cannonHenryDraperCatalogue1918}—a century-old counterpart that cataloged roughly 200,000 stars—the explosion in data is striking.

The leap from hundreds of thousands of stars in the Henry Draper Catalogue to billions in Gaia represents more than a change in scale: it is a qualitative transformation in what science becomes possible. With the Draper Catalogue, astronomers could classify stellar spectra and trace broad patterns in stellar populations. With Gaia, it is now possible to reconstruct the full three-dimensional structure and kinematics of the Milky Way, identify hypervelocity stars, and test theories of Galactic evolution with unprecedented detail. Where older surveys allowed the identification of a few rare stellar types, modern surveys allow systematic searches for extreme outliers across billions of objects, transforming the statistical character of astronomy.

This data deluge makes QA indispensable. Large-scale surveys now span the entire electromagnetic spectrum, from radio (e.g., LOFAR, ALMA) to X-ray and gamma-ray observatories such as Chandra and Fermi. Multi-messenger astronomy adds yet another layer, with gravitational waves detected by LIGO/Virgo and high-energy neutrinos from IceCube \citep{abbasiSearchIceCubeSubTeV2023}. Time-domain surveys such as ZTF and the upcoming LSST produce streams of transient and variable sources, producing large data volumes and data rates. Each dataset has distinct noise properties, resolutions, and systematic biases, making multi-wavelength integration a formidable statistical task. The ability to extract meaningful insights from these massive datasets is crucial for advancing our understanding of the universe.

\subsection{Statistical Challenges in Modern Astrophysics}

Two recurring types of statistical challenges emerge across astrophysics. The first challenge is that noisy, incomplete, and often degenerate data have uncertain theoretical statistical distributions. In addition, the number, complexity, and degeneracy of physical parameters poses major challenges \citep{schaferFrameworkStatisticalInference2015}. For example, in exoplanet studies, radial velocity measurements can only determine a planet’s minimum mass because they do not contain orbital inclination \citep{lovisRadialVelocityTechniques2010}. In cosmology, measurements of the cosmic microwave background couple the effects of the Hubble constant, the amount of ordinary matter, and dark energy, so isolating any one factor requires prior assumptions about the others \citep{christensenParameterEstimationGravitational2022}. Even within galaxies, rotation curve studies must weight the balance between visible stars and invisible dark matter. Each of these cases requires careful statistical treatment to avoid misleading conclusions.

The second challenge is that the large volume of data and model complexity creates equally daunting problems of computing time and power. As noted above, the Rubin Observatory LSST will generate tens of terabytes of imaging data per night \citep{borneAstroinformatics21stCentury2009}, while Gaia has already released petabyte-scale catalogs \citep{gaiacollaborationGaiaDataRelease2023}. Brute-force exploration of parameter spaces is simply impossible at these scales. Efficient algorithms and scalable statistical methods are required to render analysis computationally tractable \citep{huijseComputationalIntelligenceChallenges2014}. Together, these issues create a need for QA frameworks that can handle degeneracy in complex parameter spaces, model complexities, and scale efficiently with massive datasets.

\subsection{Bayesian Inference and MCMC in Context}

In this domain, Bayesian inference via Markov Chain Monte Carlo (MCMC) methods naturally emerges as a potential solution. Bayesian inference offers a principled framework for parameter estimation in complex systems. MCMC methods provide an effective way to explore parameter spaces by sampling from posterior distributions. This has become one of the most widely used and versatile approaches \citep{vontoussaintBayesianInferencePhysics2011}. Computational models are becoming far more complex, and the data being analyzed is often noisy and incomplete. Bayesian methods, with their ability to incorporate prior parameter constraints and handle uncertainty, are particularly well-suited to these challenges. MCMC methods, in particular, provide a practical way to sample from complex posterior distributions that arise in Bayesian analysis. This makes them invaluable for parameter estimation, model comparison, and uncertainty quantification.

%Flesh out this paragraph more
Another reason for the appeal of Bayesian methods is their contrast with frequentist approaches. Frequentist methods, which were dominant through much of the 20th century, emphasize point parameter estimates and confidence intervals derived from repeated sampling arguments \citep{trottaBayesSkyBayesian2008}. Bayesian inference, in contrast, provides full posterior probability distributions for parameters, naturally incorporating prior information from physics or earlier observations. This framework is particularly powerful where data are sparse, noisy, and incomplete.

The wider adoption of Bayesian statistics in astronomy gained momentum in the late 20th century. Cosmologists have applied Bayesian methods to the cosmic microwave background to extract cosmological parameters from noisy sky maps \citep{tegmarkKarhunenLoeveEigenvalueProblems1997}. In exoplanet science, Bayesian inference became common in the 1990s and 2000s for modeling radial velocity curves and transit signals in the era of larger and more precise datasets \citep{gregoryBayesianAnalysisExtrasolar2005}. Pulsar timing, supernova cosmology, and gravitational lens modeling similarly saw wider use of Bayesian methods. The trend reflects a broader recognition that many of astronomy’s hardest problems demand not just point estimates, but principled uncertainty quantification.

We review here Bayesian inference and MCMC within an astrophysical perspective because of their flexibility, principled uncertainty quantification, and growing ubiquity across astrophysics. In \hyperref[sec:Methodology]{Section II}, the methodology section develops a working foundation: here we review Bayesian statistics, priors, and likelihood construction in realistic astronomical settings. We cover toy examples before escalating to domain-relevant formulations. Then, we introduce Monte Carlo methods and build to Markov Chain Monte Carlo, deriving the Metropolis family and related samplers, providing step-by-step Python implementations on simple problems.

\hyperref[sec:CaseStudies]{Section III} presents three focused case studies that illustrate how Bayesian–MCMC pipelines advance frontiers in different subfields. For exoplanet direct detection, the section outlines the observational context and the central challenge of disentangling planetary signals from stellar activity and disk structure in direct imaging. It then outlines joint stellar–planet modeling with Gaussian processes and Bayesian classification frameworks for robust detections and non-detections, highlighting benefits and failure modes in low SNR regimes. For CMB parameter estimation, we consider geometric degeneracies in the power spectrum and shows how MCMC accelerators and parallelizable frameworks enable efficient exploration and evidence calculations, including tradeoffs between sampler sophistication and wall-clock efficiency. For gravitational-wave inference, we consider waveform fitting in high-dimensional parameter spaces, discuss the cost of likelihood evaluations, and motivate gradient-informed MCMC strategies that reduce computation without compromising accuracy.

Finally, in \hyperref[placeholder]{Section IV} and \hyperref[placeholder]{V} we compare cases where Bayesian–MCMC excels with those where complementary methods are more appropriate, identify methodological gaps revealed by the case studies, and outline opportunities for future work in astrophysics and related fields that face similar statistical and computational challenges.

\section{Methodology}
\label{sec:Methodology}

\subsection{Bayesian Preliminaries} % You can also talk about the difference between Bayesian and Frequentist Stats here
The aim of Bayesian statistics is to determine $P(H|D)$, or the probability of a hypotheses $H$ being true given data $D$. A hypothesis is any statement that can be true or false, and data is any information that can be used to evaluate the hypothesis. For example, for the roll of a single die, an hypothesis $H$ would that the die roll is a 3. The data $D$ is the result of the die roll.

Hypotheses live in the \textit{hypothesis space}, which is the set of all possible hypotheses of a system \citep{brewer1BayesianInference2018}. For the die example, the hypothesis space is $\{1,2,3,4,5,6\}$. The hypothesis space will also have a probability distribution, or a \textit{prior}, written as $P(H)$ \citep{brewer1BayesianInference2018}. The prior is the probability of each hypothesis being true absent any data. For a fair die, the prior is uniform: $\frac{1}{6}$ for all $H$ in the hypothesis space. In other words, $P(H)$ is the probability of the hypothesis being true.

The data $D$ also has a probability distribution called the \textit{evidence}, $P(D)$ \citep{brewer1BayesianInference2018}. The evidence is the probability of the data absent any hypothesis. In the die example, $P(D)$ is $\frac{1}{6}$ for rolling a $3$ on a fair die. The evidence lives in the \textit{data space}, which is the set of all possible data outcomes. In the die example, the data space is also $\{1,2,3,4,5,6\}$.

A \textit{likelihood}, $P(D|H)$, is the probability of the data assuming that the hypothesis is true \citep{brewer1BayesianInference2018}. In the die example, if $H$ is that the die roll is a $3$, then $P(D|H)$ is unity if the die roll is a $3$ and zero otherwise. We use the prior, evidence, and likelihood to calculate $P(H|D)$, which is the probability of a hypothesis being true given the data. This is called the \textit{posterior} \citep{brewer1BayesianInference2018}. In the die example, if you roll a $3$, then $P(H|D)$ is unity if the hypothesis $H$ is that the die roll is a $3$. It is zero otherwise.

The posterior $P(H|D)$ is defined by Bayes' Theorem. It can be derived using two rules, as outlined in \citet{coxProbabilityFrequencyReasonable1946}. Firstly, the probability that a hypothesis is true $P(H)$ and the probability that it is not true $P(\tilde{H})$ add up to unity:

\begin{equation}
  P(H) + P(\tilde{H}) = 1.
\end{equation}

The second rule is the product rule for conditional probabilities, which states:

\begin{equation}
  P(H)P(D|H) = P(D)P(H|D).
\end{equation}

This can be rearranged to give Bayes' Theorem:

\begin{equation}
  P(H|D) = \frac{P(H)P(D|H)}{P(D)}.
  \label{eq:bayes_theorem}
\end{equation}

\subsubsection{Example: The Double-Headed Coin}
For the previous example of a die roll, the data, the result of the die roll, completely determined the hypothesis. Bayesian statistics becomes more useful when the data are incomplete, as is often the case in astrophysics.

Consider five coins, four of which are fair, and one of which is double-headed. A random coin lands heads. We calculate the probability that the double headed coin was randomly selected.

The first step is to determine $H$ and $D$ from their respective spaces. The hypothesis space is $\{\text{Picked Fair},\text{Picked Double-Headed}\}$, which can be written concisely as $\{\text{fair},\text{double}\}$. We hypothesize that the double-headed coin was picked ($ H = \text{'double'}$). The data space is $\{\text{heads},\text{tails}\}$, and $D$ is 'heads'.

For four fair coins and one double-headed coin the priors are: $P(\text{fair}) = \frac{4}{5} = 0.8$ and $P(\text{double}) = \frac{1}{5} = 0.2$. The evidence is more complex to find in this case. Since the chance of flipping heads or tails includes the case that the double headed coin was picked, the evidence cannot be 50-50. We must calculate the probability of getting heads and tails across all the coins:

\begin{align*}
  P(\text{heads}) &= \frac{4(0.5)+1(1)}{5} = 0.6 \\
  P(\text{tails}) &= \frac{4(0.5)+1(0)}{5} = 0.4
\end{align*}

Note the implicit rule for any probability space:

\begin{equation}
  \sum_n P(x_n) = 1,
\end{equation}

where $x_n$ is a value in the space.

The likelihood $P(\text{heads}|\text{double})$ is unity, as it is only possible to get heads from the double-headed coin. Using Bayes' Theorem (\autoref{eq:bayes_theorem}):

\begin{align*}
  P(\text{double}|\text{heads}) &= \frac{P(\text{double})P(\text{heads}|\text{double})}{P(\text{heads})} = \frac{0.2}{0.6} = \boxed{0.\bar{3}}
\end{align*}

Before incorporating the data, the probability of picking the double headed coin was $0.2$, but by using Bayesian statistics, including information from the data increases the probability to $0.\bar{3}$. This example demonstrates the power of Bayesian statistics for incomplete data.

\subsection{Bayesian Statistics in Astrophysics}

In an astrophysical context, Bayes' Theorem is used to calculate the probability of \textit{parameters}, $\theta$, of a model rather than a hypothesis \citep{brewer1BayesianInference2018}.

\begin{equation}
  P(\theta|D) = \frac{P(\theta)P(D|\theta)}{P(D)}.
  \label{eq:bayes_theorem_params}
\end{equation}

Consider measurements of the radial velocity of a star over time. When a start hosts an orbiting planet, both bodies orbit a common center of mass. This induces a wobble in the star's motion which is detected as periodic Doppler shifts in the star's spectrum, also known as the \textit{radial velocity}. The radial velocity $v_r(t)$ of a star with a single planet in a strictly circular orbit can be modeled by the following equation \citep{lovisRadialVelocityTechniques2010}:

\begin{equation}
  v_r(t) = K \sin\left(\frac{2\pi t}{T} + \phi\right),
  \label{eq:rv_model}
\end{equation}

where $K$ is the velocity semi-amplitude, $T$ is the orbital period, and $\phi$ is an orbital phase offset. Thus, the parameters of this model are $\theta = \{K,P,\phi\}$. This is simplified to omit the parameters for eccentricity, system velocity, and argument of periapsis.

Consider the simulated data for radial velocity of a star shown in \autoref{fig:rv_data}. Bayes' Theorem (\autoref{eq:bayes_theorem_params}) becomes:

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{../scripts/2.2/figures/data.png}
\caption{Simulated radial velocity data of a star with a single orbiting planet (\autoref{eq:rv_model}). Each point has an error of three standard deviations. The data are noisy and incomplete, making it difficult to determine the parameters of the model.}
\label{fig:rv_data}
\end{figure}

\begin{equation}
  P(K,T,\phi|D) = \frac{P(K,T,\phi)P(D|K,T,\phi)}{P(D)}.
\end{equation}

Assuming for simplicity that the parameters are independent, the prior can be written as:

\begin{equation}
  P(K,T,\phi) = P(K)P(T)P(\phi).
\end{equation}

Absent any information, we assume the priors are uniformly distributed in a certain range. From inspection, the velocity semi-amplitude $K$ is between 5 and 15 m/s, the period $T$ is between 20 and 30 days, and the phase offset $\phi$ is between 0 and $2\pi$. Thus, the adopted uniform priors are:

\begin{equation*}
  P(K) = \begin{cases}
    \frac{1}{10} & 5 < K < 15 \\
    0 & \text{otherwise}
  \end{cases} \qquad
  P(T) = \begin{cases}
    \frac{1}{10} & 20 < T < 30 \\
    0 & \text{otherwise}
  \end{cases} \qquad
  P(\phi) = \begin{cases}
    \frac{1}{2\pi} & 0 < \phi < 2\pi \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}

and the combined prior is then:

\begin{equation}
  P(K,T,\phi) = \begin{cases}
    \frac{1}{200\pi} & 5 < K < 15, 20 < T < 30, 0 < \phi < 2\pi \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

The likelihood, $P(D|K,T,\phi)$, is the probability of the data given the parameters. For a given set of parameters, the model prediction is the mean of a normal distribution with standard deviation equal to the error of each data point. This provides the probability $P_i$ of obtaining a data point $d_i$ at time $t_i$. The likelihood for the full dataset is:

\begin{equation}
  P(D|K,T,\phi) = \prod_{i=1}^{N} P_i
\end{equation}

This takes the form:

\begin{equation}
  P(D|K,T,\phi) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left(-\frac{(D_i - v_r(t_i;K,T,\phi))^2}{2\sigma_i^2}\right)
\end{equation}

As a probability, the posterior integrated over the entire parameter space must equal unity. Thus, the evidence $P(D)$ can be found as follows:

\begin{align}
  1 &= \iiint P(K,T,\phi|D) dK dT d\phi\nonumber \\
  1 &= \iiint \frac{P(K,T,\phi)P(D|K,T,\phi)}{P(D)} dK dT d\phi \nonumber \\
  \therefore P(D) &= \iiint P(K,T,\phi)P(D|K,T,\phi) dK dT d\phi
\end{align}

This integral is analytically intractable, so it must be calculated numerically.

Using the data in \autoref{fig:rv_data}, the posterior distribution can be calculated using the code in \autoref{appx:A}. The code uses the log-prior and log-likelihood to avoid any underflow errors. The posterior can be calculated by adding the log-prior and log-likelihood, and then exponentiating the result, then normalizing with the evidence.

\begin{equation}
  P(K,T,\phi|D) = \frac{e^{\log(P(K,T,\phi)) + \log(P(D|K,T,\phi))}}{P(D)}
\end{equation}

The program evaluates the posterior on a grid of 100 points for each parameter, giving a total of $100^3 = 1,000,000$ points. The posterior is also cumulatively summed at every evaluated point, giving a numerical estimate of the evidence $P(D)$. Finally, the marginal posteriors of each parameter are calculated from the total posterior by integrating over the other two parameters:
\begin{align}
  P(K|D) &= \iint P(K,T,\phi|D) dT d\phi \\
  P(T|D) &= \iint P(K,T,\phi|D) dK d\phi \\
  P(\phi|D) &= \iint P(K,T,\phi|D) dK dT
\end{align}
The resulting posterior distributions are shown in \autoref{fig:bf_posteriors}.

\begin{figure}[ht!]
\centering

\subfigure[$\mu = 9.99$ m/s $\quad \sigma = 0.73$ m/s]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/k-posterior.png}%
    \label{fig:k_posterior}%
}
\hfill
\subfigure[$\mu = 21.72$ days $\quad \sigma = 0.44$ days]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/t-posterior.png}%
    \label{fig:t_posterior}%
}
\hfill
\subfigure[$\mu = 0.89$ rad $\quad \sigma = 0.13$ rad]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.2/figures/phi-posterior.png}%
    \label{fig:phi_posterior}%
}
\caption{The marginal posterior distributions of the parameters $K$, $T$, and $\phi$ given the data in \autoref{fig:rv_data}. These posteriors are normalized with a numerically calculated evidence.}
\label{fig:bf_posteriors}
\end{figure}

The exact parameters of a model are always unknown, but posterior distributions field a close estimate. The true parameters, $K=10$ m/s, $T=21.4$ days, and $\phi=\frac{\pi}{4}$, fall within one standard deviation of the mean of each posterior distribution (\autoref{fig:bf_posteriors}), indicating that the Bayesian method was successful in estimating the parameters.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{../scripts/2.2/figures/prediction.png}
\caption{The radial velocity data from \autoref{fig:rv_data} along with the mean prediction from the posterior distributions in \autoref{fig:bf_posteriors}. The shaded region represents one standard deviation from the mean prediction.}
\label{fig:prediction}
\end{figure}

\autoref{fig:prediction} shows the true model and the mean parameter prediction from the posterior distributions. Bayes' Theorem is thus useful to estimate the parameters for astrophysical models. However, this method is computationally expensive. Computing the posterior numerically requires integrating over the full parameter space. For $n$ parameters and $m$ possible values for each parameter, this algorithm would have a time complexity of $O(m^n)$, which is infeasible for even small $n$ and $m$. To solve this problem, we can use \textit{Monte Carlo methods} \citep{brewer1BayesianInference2018}.

\subsection{Monte Carlo Sampling}
As noted above, the primary computational challenge in brute force Bayesian inference arises from evaluating the posterior probability at every point in the parameter space. One way to reduce this cost is to sample only a subset of points, distributed according to the posterior itself \citep{vontoussaintBayesianInferencePhysics2011}. These points are called \textit{draws}. These draws make it possible to calculate expectation values from the explicit posterior using only the sampled points \citep{vontoussaintBayesianInferencePhysics2011}. This defines \textit{Monte Carlo sampling}, which can be used to approximate integrals and expectations that would otherwise require explicit evaluation over a continuous space.

For a known probability density, the expectation value of some variable $x$ can be calculated as:

\begin{equation}
  \langle x \rangle = \int x P(x|\theta) dx.
\end{equation}

For $N$ values of $x$ drawn from $P$ using Monte Carlo, this can be discretely approximated as:

\begin{equation}
  \langle x \rangle \approx \frac{1}{N} \sum_{i=1}^{N} x_i,
\end{equation}
which generalizes naturally to higher-dimensional parameter spaces, though convergence can become slower as dimensionality increases \citep{vontoussaintBayesianInferencePhysics2011}.

$x$ is a function of the parameters, and can be defined arbitrarily to calculate the expectation value of any value of interest. Given a probability density $P(\theta_1,\theta_2,\hdots,\theta_n)$, some forms of $x$ are listed in \autoref{tab:x_forms}.
\begin{deluxetable}{ll}
  \tablewidth{0pt}
  \tablenum{1}
  \tablehead{
    \colhead{$x$} & \colhead{Description}
  }
  \startdata
    $\theta_i$ & Expectation value of parameter $\theta_i$ \\
    $\theta_i^2$ & Variance of parameter $\theta_i$ \\
    $f(\theta_1,\theta_2,\hdots,\theta_n)$ & Expectation value of any function of the parameters \\
    $\mathds{1}(\theta_i = a)$ & Probability that $\theta_i = a$ \\
    $\mathds{1}(a \le \theta_i \le b)$ & Probability that $\theta_i$ is in the range $[a,b]$ \\
    $\mathds{1}(\text{condition})$ & Probability that the condition is true \\
  \enddata
  \caption{Forms of $x$ to calculate expectation values of various quantities.}
  \label{tab:x_forms}
\end{deluxetable}

The usefulness of Monte Carlo sampling becomes apparent when considering the computation of marginal posteriors. In the previous example, we used the posterior to calculate the marginal posteriors of each parameter. In the brute force method, this was found by integrating the posterior over all other parameters. Monte Carlo makes this simpler. If we have a set of $N$ draws from the posterior of a parameter space $\{\theta_1,\theta_2,\hdots,\theta_n\}$, we can obtain draws for a certain parameter $\theta_i$ by ignoring all other parameters \citep{brewer1BayesianInference2018}. A histogram of the draws of $\theta_i$ then gives the marginal posterior of $\theta_i$. This is much simpler than integrating over all other parameters, especially in high-dimensional parameter spaces.

\subsection{Markov Chain Monte Carlo}
\textit{Markov Chain Monte Carlo} (MCMC) is one of the many methods used to obtain posterior draws, and in modern astrophysics, has become a standard tool for Bayesian inference. Using MCMC, the posterior of even a high-dimensional parameter space can computed efficiently \citep{trottaBayesSkyBayesian2008}. MCMC does this by creating a sequence of draws in the form of a \textit{Markov Chain} \citep{neal1993probabilistic}.

\subsubsection{Markov Chains}

A Markov Chain is a sequence of random variables, $x_0, x_1, \hdots, x_n$, where the probability of each variable only depends on the previous variable \citep{neal1993probabilistic}. This is known as the \textit{Markov property}, and can be written mathematically as:
\begin{equation}
  P(x_{n+1}|x_n,x_{n-1},\hdots,x_0) = P(x_{n+1}|x_n).
\end{equation}

To create a Markov chain, we must first decide an initial value $x_0$ using an initial probability distribution $P_0(x_0)$. The probability distribution of the next value is evolved iteratively from the previous probability distribution using a \textit{transition probability} \citep{neal1993probabilistic} $T_n(x_n,x_{n+1})$:
\begin{equation}
  P(x_{n+1}) = P(x_n)T_n(x_n,x_{n+1})
\end{equation}

If the transition probability does not depend on the point in the chain, the chain is \textit{homogenous}, or \textit{stationary} \citep{neal1993probabilistic}.

Markov chains can be shown to converge to a \textit{stationary distribution} $P^*(x)$, which is independent of the initial distribution \citep{trottaBayesSkyBayesian2008}. A stationary distribution, once reached, does not change as the chain evolves further.

Markov chains can also be \textit{ergodic} \citep{neal1993probabilistic}, which means that a stationary distribution can be reached from any initial distribution. A ergodic chain is irreducible and aperiodic \citep{neal1993probabilistic}. Irreducibility means that it is possible to reach any point in the space from any other point, and aperiodicity means that the chain does not get stuck in cycles \citep{vontoussaintBayesianInferencePhysics2011}.

For Bayesian inference, the goal of MCMC is to construct a Markov chain whose stationary distribution is the unknown posterior distribution \citep{brewer1BayesianInference2018}. To ensure that the chain converges to the desired posterior, the choice of transition probability $T(x_n, x_{n+1})$ is crucial. The transition rule must satisfy the condition of \textit{detailed balance}, which guarantees that, at equilibrium, the probability flow between any two states is symmetric:
\begin{equation}
  P(x_n)T(x_n, x_{n+1}) = P(x_{n+1})T(x_{n+1}, x_n).
\end{equation}
This condition ensures that the chain does not drift away from the target distribution once it has been reached \citep{vontoussaintBayesianInferencePhysics2011}.

\subsubsection{The Metropolis-Hastings Algorithm}

One of the simplest and most widely used algorithms that satisfies detailed balance is the \textit{Metropolis algorithm} \citep{metropolisEquationStateCalculations1953}, more specifically, its generalization, the \textit{Metropolis–Hastings algorithm} \citep{hastingsMonteCarloSampling1970}. The algorithm constructs a Markov chain by iteratively proposing new states in the parameter space and deciding whether to accept or reject them based on the target distribution. \citet{brewer1BayesianInference2018} outlines the steps of the Metropolis-Hastings algorithm as follows:

\begin{enumerate}
  \item Initialize the chain with a starting point $x_0$.
  \item Generate a candidate point $x_{n+1}$ from a proposal distribution $q(x_{n+1}|x_n)$ based on the current state $x_n$.
  \item Accept or reject the proposal
  \item Repeat steps 2 and 3 for a large number of iterations to generate a sequence of samples until convergence.
\end{enumerate}

The algorithm depends on the proposal distribution $q(x_{n+1}|x_n)$ and the acceptance criterion. Commonly, a 'random walk' proposal is used to generate candidates by adding a small random perturbation to the current state. For an $n$-dimensional parameter space, the random walk proposal is a symmetric (usually Gaussian) distribution, also of $n$-dimensions, with the current state as its mean \citep{vontoussaintBayesianInferencePhysics2011}. If the proposal distribution is indeed symmetric, then the following is implied:
\begin{equation}
  \label{eq:symmetric_proposal}
  q(x_{n+1}|x_n) = q(x_n|x_{n+1}).
\end{equation}
In this case, the Metropolis-Hastings algorithm simplifies to the original Metropolis algorithm \citep{brewer1BayesianInference2018}. Asymmetric proposal distributions are more general, and can also be used. However, they are less common in practice due to their specificity to the problem.

The acceptance criterion is based on the ratio of the posterior at the proposed and current states, adjusted by the proposal distribution. This is the probability of the proposal being accepted. Mathematically, this is \citep{brewer1BayesianInference2018}:
\begin{equation}
  \alpha = \min\left(1, \frac{P(x_{n+1}|D) q(x_n|x_{n+1})}{P(x_n|D) q(x_{n+1}|x_n)}\right) = \min\left(1, \frac{P(D|x_{n+1}) P(x_{n+1}) q(x_n|x_{n+1})}{P(D|x_n) P(x_n) q(x_{n+1}|x_n)}\right).
\end{equation}
If the proposal distribution is symmetric, we can further apply \autoref{eq:symmetric_proposal} to obtain \citep{brewer1BayesianInference2018}:
\begin{equation}
  \alpha = \min\left(1, \frac{P(D|x_{n+1}) P(x_{n+1})}{P(D|x_n) P(x_n)}\right).
\end{equation}
Simplified with log-priors and log-likelihoods:
\begin{equation}
  \log(\alpha) = \min\left(0, \log(P(D|x_{n+1})) + \log(P(x_{n+1})) - \log(P(D|x_n)) - \log(P(x_n))\right).
\end{equation}

This criterion accepts any proposal that increases the posterior probability, while proposals that decrease it are accepted with a probability proportional to the decrease. If a proposal is rejected, the current state is counted again. This allows the chain to sample high-probability regions more frequently while still having the ability to escape local maxima and explore the full parameter space \citep{brewer1BayesianInference2018}.

The choice of proposal distribution can also significantly affect the efficiency of the algorithm \citep{vontoussaintBayesianInferencePhysics2011}. If the proposal steps are too small, the chain will explore the parameter space slowly, leading to high correlation between samples. Conversely, if the steps are too large, many proposals will be rejected, also resulting in inefficient sampling. Tuning the proposal distribution to balance exploration and acceptance rates is often necessary for optimal performance \citep{vontoussaintBayesianInferencePhysics2011}. A common heuristic is to adjust the proposal distribution to achieve an acceptance rate between $0.2$ and $0.5$, depending on the dimensionality of the parameter space \citep{gelmanWeakConvergenceOptimal1997}.

The Metropolis-Hastings algorithm has a \textit{burn-in} period. The initial samples of the chain may not be representative of the target distribution, especially if the starting point is far from high-probability regions. To mitigate this, it is common practice to discard a certain number of initial samples, from the burn-in period, before using the remaining samples for inference \citep{vanravenzwaaijSimpleIntroductionMarkov2018}.

The Metropolis-Hastings algorithm outputs a Markov chain of samples that approximate the target posterior distribution \citep{hastingsMonteCarloSampling1970}. Since the probability is only calculated for proposed points instead of the full parameter space, the algorithm is much more computationally efficient than brute force methods. The time complexity is reduced to $O(n*m)$, where $n$ is the number of samples drawn and $m$ is the number of walks, which is feasible for even high-dimensional parameter spaces.

\subsection{MCMC in Astrophysics}

Revisiting the radial velocity example, we can use MCMC to obtain draws from the posterior distribution of the parameters $\{K,T,\phi\}$ from \autoref{eq:rv_model} given the data in \autoref{fig:rv_data}. Using the Metropolis-Hastings algorithm, 10 chains were run, each with 12,000 samples. The first 1,000 samples of each chain were discarded as burn-in. The code used to run the MCMC is given in \autoref{appx:B}.

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{../scripts/2.5/figures/traces.png}
\caption{Trace plots of the Markov chains for each parameter. A total of 10 chains were run, each with 12,000 samples. The first 1,000 samples of each chain were discarded as burn-in.}
\label{fig:mcmc_trace}
\end{figure}

The trace plots of the chains are shown in \autoref{fig:mcmc_trace}. These are visualizations of the Markov chains for each parameter. The chains appear to be well-mixed and stationary, indicating that they have converged to the target distribution. The marginal posterior distributions of each parameter is shown in \autoref{fig:mcmc_posteriors}.

\begin{figure}[ht!]
\centering

\subfigure[$\mu = 10.02$ m/s $\quad \sigma = 0.73$ m/s]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/k-posterior.png}%
    \label{fig:k_mcmc_posterior}%
}
\hfill
\subfigure[$\mu = 21.71$ days $\quad \sigma = 0.43$ days]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/t-posterior.png}%
    \label{fig:t_mcmc_posterior}%
}
\hfill
\subfigure[$\mu = 0.89$ rad $\quad \sigma = 0.13$ rad]{%
    \includegraphics[width=0.32\textwidth]{../scripts/2.5/figures/phi-posterior.png}%
    \label{fig:phi_mcmc_posterior}%
}
\caption{The histograms of the draws from the posterior distributions of the parameters $K$, $T$, and $\phi$ given the data in \autoref{fig:rv_data}.}
\label{fig:mcmc_posteriors}
\end{figure}

Comparing these posteriors to the ones obtained using the brute-force method, we see that they are very similar. \autoref{tab:comparison} summarizes the comparison between the true parameter values, the estimates from the brute-force method (\autoref{fig:bf_posteriors}), and the estimates from the MCMC method (\autoref{fig:mcmc_posteriors}). The two algorithms are compared using the mean and standard deviation of each posterior distribution, as well as the Z-score of the true value from the mean of each posterior distribution. The Z-score is a measure of how many standard deviations a value is from the mean, and is calculated as:
\begin{equation}
  Z = \frac{X - \mu}{\sigma},
\end{equation}
where $X$ is the value being compared, $\mu$ is the mean of the distribution, and $\sigma$ is the standard deviation of the distribution. A Z-score of zero indicates that the value is equal to the mean, while a Z-score of 1 indicates that the value is one standard deviation above the mean. A Z-score of -1 indicates that the value is one standard deviation below the mean. A Z-score within the range of -2 to 2 is generally considered to be acceptable, as it indicates that the value is within two standard deviations of the mean.
\begin{deluxetable}{llllll}
  \tablewidth{0pt}
  \tablenum{2}
  \tablehead{
    \colhead{Parameter} & \colhead{True} & \colhead{Brute-Force} & \colhead{Brute-Force Z-score} & \colhead{MCMC}  & \colhead{MCMC Z-score}
  }
  \startdata
    $K$ (m/s) & $10.00$ & $9.99 \pm 0.73$ & -0.01 & $10.02 \pm 0.73$ & 0.03 \\
    $T$ (days) & $21.4$ & $21.72 \pm 0.44$ & 0.68 & $21.71 \pm 0.43$ & 0.66 \\
    $\phi$ (rad) & $0.25\pi$ & $0.89 \pm 0.13$ & 0.77 & $0.89 \pm 0.13$ & 0.77 \\
  \enddata
  \caption{Comparison of the true parameter values, the estimates from the brute-force method, and the estimates from the MCMC method.}
  \label{tab:comparison}
\end{deluxetable}

Both methods give very similar results (\autoref{tab:comparison}). The Z-scores of the true values are all within 1, indicating that both methods were successful in estimating the parameters. The MCMC method, however, was far more computationally efficient. As described above, the brute-force method had to evaluate the posterior at $100^3 = 1,000,000$ points in the parameter space. The MCMC method, as implemented in \autoref{appx:B}, only had to evaluate the posterior at $10 \times 12,000 = 120,000$ points, a reduction of nearly an order of magnitude, to provide virtually the same results.

Generalizing this to higher-dimensional parameter spaces, the computational savings of MCMC become even more apparent. In a hypothetical 10-dimensional parameter space with 100 possible values for each parameter, the brute-force method would have to evaluate the posterior at $100^{10} = 10^{20}$ points, which is infeasible. The MCMC method, on the other hand, would still provide reasonably accurate results with no significant increase in computing cost. The number of required samples and walks increases with the number of dimensions based on the discretion of the experimenter. Usually, this increase is sub-exponential, even linear with respect to an $n$-dimensional parameter space.

\subsection{Applications in Research}

Using Bayesian inference and MCMC in astrophysics is far more complex than the radial velocity example given above. Real-world astrophysical models often have many more parameters, complex likelihood functions, and intricate prior distributions. In practice, astrophysical analyses must contend with challenges such as correlated noise in observations, hierarchical modeling of populations, and significant computational bottlenecks when scaling to high-dimensional or large datasets. The adaptability of the Bayesian–MCMC framework makes it useful in these conditions to tackle problems of parameter estimation, model selection, and uncertainty quantification across various astrophysical subfields.

In the following sections, we will explore some case studies where Bayesian inference and MCMC have been successfully applied, including exoplanet detection, cosmological parameter estimation, and gravitational wave data analysis.

\section{Direct Imaging of Exoplanets}
\label{sec:CaseStudies}

\subsection{Introduction}

Exoplanets are planets that orbit stars outside our solar system, with each being a unique laboratory for studying planetary formation and evolution \citep{kaushikExoplanetDetectionDetailed2025}. Studying their atmospheres, compositions, and orbital dynamics can provide insights into the conditions necessary for life and the diversity of planetary systems in the universe. These properties are key to understanding not only the planets themselves but also the broader processes that govern planetary system formation and evolution \citep{madhusudhanExoplanetaryAtmospheresKey2019}.

Exoplanets are detected using various methods, including transit photometry, radial velocity measurements, direct imaging, and other less common techniques \citep{weiSurveyExoplanetaryDetection2018}. Each method probes a different region of exoplanet parameter space, with transit and radial velocity methods being most sensitive to close-in planets, while direct imaging is sensitive to young, massive planets at wide separations \citep{fischerExoplanetDetectionTechniques2014b}. Of these, direct imaging is the most straightforward in concept, as it does not rely on indirect signatures. The brightness of an exoplanet, whether reflected starlight or thermal emission, is directly imaged by a telescope \citep{kaushikExoplanetDetectionDetailed2025}. This approach enables direct spectroscopic analysis, offering unique access to the planet’s atmospheric composition, temperature, and cloud structure. Although the yield of direct imaging is lower than that of indirect methods, directly imaged exoplanets constitute a crucial subset of targets for atmospheric and dynamical characterization \citep{currieDirectImagingSpectroscopy2023}.

Direct imaging has achieved several notable successes since its first confirmed detections in the early 2000s. The HR 8799 system, imaged by \citet{maroisDirectImagingMultiple2008}, revealed four massive exoplanets orbiting a young A-type star, marking a milestone in multi-planet imaging. Other examples include $\beta$ Pictoris b \citep{lagrangeGiantPlanetImaged2010} and 51 Eridani b \citep{macintoshDiscoverySpectroscopyYoung2015}, both of which have become benchmark systems for understanding the atmospheres of young gas giants. More recently, the James Webb Space Telescope (JWST) \textbf{[ADD FOOTNOTE]} and ground-based instruments such as SPHERE \textbf{[ADD FOOTNOTE]} on the Very Large Telescope (VLT) \textbf{[ADD FOOTNOTE]} and GPI \textbf{[ADD FOOTNOTE]} on Gemini South have achieved higher contrasts and enabled spectroscopy of fainter, cooler companions such as HIP 65426 b \citep{carterJWSTEarlyRelease2023}.

However, direct imaging faces substantial observational and technical challenges. The primary limitation arises from the extreme contrast between the stellar and planetary brightness: even a bright exoplanet is typically $10^{9}$ times fainter than its host star \citep{fischerExoplanetDetectionTechniques2014b}. A dominant obstacle is also the \textit{point spread function} (PSF), which is the diffraction pattern created by the telescope optics \citep{fischerExoplanetDetectionTechniques2014b}. The PSF spreads starlight over the detector and can easily obscure faint companions \citep{fischerExoplanetDetectionTechniques2014b}. PSFs come in various shapes, ranging from circular Airy rings to more complex forms depending on the specifics of the telescope aperture \citep{fischerExoplanetDetectionTechniques2014b}. Various image processing techniques are employed to enhance the visibility of the exoplanet against the stellar background. These techniques can be broadly categorized into \textit{preprocessing} and \textit{postprocessing} methods.

\subsubsection{Preprocessing methods}

Preprocessing methods aim to reduce the intensity of the central PSF and improve the contrast between the star and the exoplanet. This was first performed using a \textit{coronagraph} \citep{lyotStudySolarCorona1939}. This optical device is placed in the light path of the telescope to block out the light from the central star, allowing the much dimmer light from surrounding objects, such as exoplanets, to become more promiment. This reduces the intensity of the central star by up to 2 orders of magnitude \citep{chauvinDirectImagingExoplanets2023}.

Changing the shape of the telescope aperture can also advantageously change the shape of the PSF. \citet{zanoniReductionDiffractedLight1965} argue for a square aperture, thereby constaining the PSF to the \textit{x} and \textit{y}-axes and suppressing the diagonals, which reduce the intensity of diffracted light from the central star by four orders of magnitude.

Modern preprocessing methods include \textit{apodization}, which is a technique to reduce the brightness of the PSF without blocking out the host star. This is accomplished by altering the shape and transmission of a telescope aperture \citep{nisensonDetectionEarthlikePlanets2001}. A number of apodization techniques are used, ranging from pupil-plane masks \citep{reddyApodizationPupilsDesign2018}, to phase-induced approaches \citep{guyonExoplanetImagingPhaseinduced2005}.

\subsubsection{Postprocessing methods}

Once an image has been acquired, postprocessing methods play a crucial role in distinguishing faint planetary signals from residual starlight. These techniques operate on the recorded data to remove the effect of the PSF, revealing features masked by the intense starlight \citep{lafreniereNewAlgorithmPoint2007}. A simple approach is \textit{reference differential imaging} (RDI), which involves taking images of a reference star with similar properties to the target star. The reference image is then subtracted from the target image to remove the stellar contribution \citep{folletteIntroductionHighContrast2023}. However, in the presence of temporal variations in the atmosphere and instrument, RDI may not accurately capture the PSF variations, leading to residual artifacts.

To account for this, more advanced techniques such as \textit{angular differential imaging} (ADI) \citep{maroisAngularDifferentialImaging2006} and \textit{spectral differential imaging} (SDI) \citep{folletteIntroductionHighContrast2023} have been developed. ADI leverages the rotation of the field of view during observations to distinguish between static PSF features and rotating planetary signals \citep{maroisAngularDifferentialImaging2006}. SDI, on the other hand, exploits the spectral differences between the star and planet by observing at multiple wavelengths, allowing for differential subtraction based on color contrasts \citep{folletteIntroductionHighContrast2023}.

In addition to these methods, statistical PSF modeling techniques have become widely adopted. Locally Optimized Combination of Images (LOCI) \citep{lafreniereNewAlgorithmPoint2007} constructs a linear combination of reference frames that minimizes residual noise within localized regions of the image. Principal Component Analysis (PCA) or Karhunen-Loève Image Projection (KLIP) extended this concept, representing the PSF as a combination of orthogonal basis images derived from large reference libraries \citep{folletteIntroductionHighContrast2023}.

\subsection{Challenges}

Even after filtering out the stellar emission, several challenges remain in the direct imaging of exoplanets. One major challenge is the presence of \textit{speckle noise}, which arises from residual wavefront errors (microscopic imperfections in the telescope aperture) and residual refractive atmospheric \citep{fischerExoplanetDetectionTechniques2014b}. These errors create discrete speckles that can mimic the appearance of point sources, making it difficult to distinguish between true planetary signals and noise artifacts \citep{fischerExoplanetDetectionTechniques2014b}. Furthermore, overfiltering during PSF subtraction can lead to \textit{self-subtraction}, where part of the planet's signal is inadvertently removed along with the stellar PSF, leading to \textit{non-detections} \citep{kaushikExoplanetDetectionDetailed2024}. This effect is particularly pronounced for planets located close to their host stars.

Rather than relying solely on algorithmic subtraction, modern postprocessing analyses construct generative residual models of the image, allowing the use of Bayesian hypothesis testing and MCMC sampling to infer contrasts, spectra, and uncertainties \citep{ruffioBayesianFrameworkExoplanet2018}. In the next stage, we adopt a correlated-noise likelihood and physically motivated priors to evaluate planet-versus-no-planet hypotheses across the imaged field, yielding posterior detection maps and upper limits that are robust to speckle statistics and algorithmic throughput losses.

\appendix
\section[Appendix A]{Radial Velocity Brute-Force Approach}
\label{appx:A}

\lstinputlisting[style=pythonstyle]{../scripts/2.2/brute-force-ex.py}

\section[Appendix B]{Radial Velocity MCMC Approach}
\label{appx:B}

\lstinputlisting[style=pythonstyle]{../scripts/2.5/mcmc-ex.py}

\bibliography{references}

\end{document}
