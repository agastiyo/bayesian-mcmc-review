\documentclass[preprint2,longauthor]{aastex631}
\usepackage{blindtext}

% Note to self: Don't forget to start up Zotero when you start writing!

\begin{document}
\label{placeholder} % Delete this when done!

\title{Bayesian Inference and MCMC Methods in Astrophysics}
\author{Agastya Gaur}
\affiliation{University of Illinois at Urbana-Champaign}

\begin{abstract}
  \blindtext
\end{abstract}

\keywords{Astrophysics, Astrostatistics, Bayesian Statistics, Markov Chain Monte Carlo, Big Data}

\section{Introduction}
\label{sec:Introduction}
\subsection{The History of Astrostatistics}
In the 4th century BC, Hipparchus, attempting to estimate the length of a year, found the middle of the range of a scattered set of Babylonian solstice measurements. An achievement for the time, Hipparchus's measurement marked the beginning of what would become a long-standing marriage between astronomy and statistics. In the centuries to come, a number of breakthroughs in astrostatistics followed. Notably, Tycho Brahe in the late 1500s employed repeated positional measurements of stars and used the mean of the data to map them. His work was so precise that it became the foundation of Kepler's laws of planetary motion, and it took astronomers generations to produce better measurements. \citep{leavesleyTychoBrahesWay2018}. Furthermore, in the 1770s, Laplace rediscovered Bayesian statistics, and over the next decade he continued to expand upon his work, using it in a colossal effort to extend Newton's theory of gravity, work that would have him hailed as a monumental genius \citep{stiglerStudiesHistoryProbability1975}.

The biggest advancement in astrostatistics before the era of computing came in 1805 when Legendre published the method of least squares regression to model the orbit of comets \citep{feigelsonStatisticalChallengesModern2004}. He theorized that the model best fit to a set of data was one that minimized the sum of the squares of the errors. Though Legendre did not provide a formal proof of the method, regarding it only as a convenient trick, later works by Robert Adrain developed formal mathematical proofs of the method \citep{merrimanHistoryMethodLeast1877}. In 1809, Gauss published his own work on least squares, showing it was used to calculate the orbit of the dwarf planet Ceres, even when observing it was impossible due to solar glare. Less elegantly, he also insisted that he had discovered the method years before Legendre \citep{stiglerGaussInventionLeast1981}. As controversial as the development of least squares regression ended up being, it has cemented itself in history as one of the most important leaps in astrostatistics.

The recurring theme was clear: progress in astronomy often hinged on solving problems of statistical estimation. By the end of the century, astronomy had firmly established itself as a quantitative science, driven by the refinement of statistical methods to identify regularities in scattered measurements, fitting orbital models, and quantifying uncertainty in the presence of noise.

\subsection{Resolving the Identity Crisis}

The next 100 years brought two developments that reshaped the relationship between astronomy and statistics: the rise of physics as the foundation of astronomy and the advent of computing, which enabled unprecedented scales of data analysis. As astronomy grew increasingly intertwined with the theories of physics, the field transformed into what we now call astrophysics. As more astronomers began to call themselves astrophysicists, statistics began to fade in prominence. Though a niche field called statistical astronomy persisted, the majority of astronomers made little use of statistics in their work \citep{feigelsonStatisticalChallengesModern2004}. The focus shifted to deriving physical models from first principles, and statistical methods were often seen as secondary or even unnecessary. \citet{hubbleDistributionLuminosityElliptical1930} determined the fit for the light curve of elliptical galaxies by trial-and-error instead of regression. \citet{zwickyMassesNebulaeClusters1937} first observed dark matter using a curve fitted only by eye.

The disdain for statistics stemmed from most astrophysicists' stubborn adherence to Newtonian determinism. Physics was regarded as the fundamental law of nature and an elegant basis for astronomy, while statistics was seen as rough, approximate, and imperfect. Around the same time, statistics flourished in the social sciences, further pushing it from astrophysics.

However, statistics would not be kept away from astronomy for long. As computers developed, astronomers increasingly adopted new tools for both calculation and simulation. In the 1920s, one of the earliest applications appeared in the production of lunar tables. Previously, astronomers calculated the position of the Moon using complex, error-prone methods that required extensive manual computation \citep{duncombeEarlyApplicationsComputer1988}. By the 1930s, however, \citet{comrieApplicationHollerithTabulating1932} demonstrated how punch card computing machines could automate the process, making lunar ephemerides faster and more reliable. From then on to the 1970s, Comrie's work was continued by Wallace Eckert, who improved the punch-card calculations using IBM computers \citep{olleyTaskThatExceeded2018}. Like lunar calculations, galactic simulations were also performed with computers as early as the 1940s. \citet{holmbergClusteringTendenciesNebulae1940} modeled gravitational interactions using lightbulbs to represent galaxies, demonstrating how spiral structures could emerge. These analog demonstrations laid the groundwork for fully computational N-body simulations in the 1970s, such as \citet{toomreGalacticBridgesTails1972}, who used them to explain tidal tails and bridges in interacting galaxies.

With these advances in computing came a natural resurgence of statistical methods in astronomy. The ability to automate calculations, handle larger datasets, and simulate complex systems meant that statistical analysis became not only feasible but indispensable. In the mid-20th century, the growth of galaxy surveys encouraged quantitative treatments of structure and dynamics. Early work such as \citet{lynden-bellStatisticalMechanicsViolent1967} applied statistical mechanics to stellar systems, laying the foundation for the study of galaxy formation and equilibrium. By the 1970s, statistics was increasingly recognized as a distinct methodological pillar of astronomy. \citet{peeblesStatisticalAnalysisCatalogs1973} systematically catlogued and analyzed extragalactic objects using power spectra and correlation functions, pioneering the statistical study of large-scale structure.

Where once statistical methods were painfully labor-intensive, computing enabled astronomers to apply them across vast amounts of observational data. While the early history of the field was dominated by statistical reasoning, the growth of physics and computation broadened this into what we now call quantitative analysis (QA). Quantitative analysis thus represents the merging of three traditions that once stood apart: the deductive rigor of physics, the inferential power of statistics, and the scalability of computation. In modern astronomy, progress often relies not on one of these strands in isolation but on their integration. QA therefore serves as both a methodological framework and a philosophy of practice, emphasizing reproducibility, uncertainty quantification, and the ability to extract physical meaning from complex data.

\subsection{The Data Deluge}

Today, astrophysics sits in the middle of a universe of complex statistical problems that demand new quantitative approaches and more computing power with each passing day. In many respects, QA has become the backbone of research in modern astrophysics, and at a pivotal moment as the 21st century has ushered in an unprecedented era of astronomical data generation. Sky surveys like Gaia DR3 alone provide astrometry and photometry for nearly two billion stars, plus more than ten million variable sources across dozens of types \citep{gaiacollaborationGaiaDataRelease2023}. The nineteenth data release of the Sloan Digital Sky Survey collected robust spectroscopic data from over 6 million objects \citep{collaborationNineteenthDataRelease2025}. Advances in CCD detectors will see data from sky surveys continue to grow in the next decade from gigabytes to terabytes today, and possibly to petabytes in the near future. The same trend can be seen in data from NASA's Solar Dynamics Observatory, which now generates over a terabyte of data per day, and the Rubin LSST, generating close to 30 terabytes per day \citep{borneAstroinformatics21stCentury2009}. Compared to the Henry Draper Catalogue \citep{cannonHenryDraperCatalogue1918}—a century-old counterpart that cataloged roughly 200,000 stars—the explosion in data is striking.

The leap from hundreds of thousands of stars in the Henry Draper Catalogue to billions in Gaia represents more than a change in scale: it is a qualitative transformation in what science becomes possible. With the Draper Catalogue, astronomers could classify stellar spectra and trace broad patterns in stellar populations. With Gaia, it is now possible to reconstruct the full three-dimensional structure and kinematics of the Milky Way, identify hypervelocity stars, and test theories of Galactic evolution with unprecedented detail. Where older surveys allowed the identification of a few rare stellar types, modern surveys allow systematic searches for extreme outliers across billions of objects, transforming the statistical character of astronomy.

This data deluge makes QA indispensable. Large-scale surveys now span the entire electromagnetic spectrum, from radio (e.g., LOFAR, ALMA) to X-ray and gamma-ray observatories such as Chandra and Fermi. Multi-messenger astronomy adds yet another layer, with gravitational waves detected by LIGO/Virgo and high-energy neutrinos from IceCube \citep{abbasiSearchIceCubeSubTeV2023}. Time-domain surveys such as ZTF and the upcoming LSST produce streams of transient and variable sources, producing data that are not only high-volume but also high-velocity. This also creates qualitatively harder problems as each modality comes with distinct noise properties, resolutions, and systematic biases, making integration across datasets a formidable statistical task. The ability to extract meaningful insights from these massive datasets in an organized manner is crucial for advancing our understanding of the universe. QA provides a number of powerful tools spanning statistical inference, computational algorithms, and machine learning methodologies to analyze, interpret, and model this data effectively.

\subsection{Statistical Challenges in Modern Astrophysics}

Across astrophysics, two recurring types of challenges emerge. The first challenge is that noisy, incomplete, and often degenerate data have estimated distributions that differ across multiple competing theories. Though theoretical astrophysics has given us the tools to reduce these problems to estimations of physical constants, the number and complexity of parameters still pose major challenges \citep{schaferFrameworkStatisticalInference2015}. Such degeneracies manifest across nearly every subfield of astrophysics. For example, in exoplanet studies, radial velocity measurements can only determine a planet’s minimum mass because they can’t reveal whether we are seeing the orbit at an angle. In cosmology, measurements of the cosmic microwave background blur together the effects of the Hubble constant, the amount of ordinary matter, and dark energy, so isolating any one factor requires assumptions about the others. Even within galaxies, rotation curve studies must weigh the balance between visible stars and invisible dark matter. Each of these cases requires careful statistical treatment to avoid misleading conclusions.

The second challenge is that the large volume of data creates equally daunting problems of computing time and power. The Rubin Observatory LSST will generate tens of terabytes of imaging data per night \citep{borneAstroinformatics21stCentury2009}, while Gaia has already released petabyte-scale catalogs \citep{gaiacollaborationGaiaDataRelease2023}. Brute-force exploration of parameter spaces is simply impossible at these scales. Efficient algorithms and scalable statistical methods are required to render analysis computationally tractable \citep{huijseComputationalIntelligenceChallenges2014}. Without such methods, the majority of information encoded in these massive datasets would remain inaccessible. Together, these issues create a need for QA frameworks that can both handle degeneracy in complex parameter spaces and scale efficiently with massive datasets.

\subsection{Bayesian Inference and MCMC in Context}

Within this landscape, Bayesian inference via Markov Chain Monte Carlo (MCMC) methods naturally emerges as a potential solution.. Bayesian inference offers a principled framework for parameter estimation in complex systems, and MCMC methods provide an effective way to explore parameter spaces by sampling from posterior distributions. For astrophysicists, this has become one of the most widely used and versatile approaches. \citet{vontoussaintBayesianInferencePhysics2011} notes the growing applicability of Bayesian inference in physics. Computational models are becoming far more complex, and the data being analyzed is often noisy and incomplete. Bayesian methods, with their ability to incorporate prior knowledge and handle uncertainty, are particularly well-suited to these challenges. MCMC methods, in particular, provide a practical way to sample from complex posterior distributions that arise in Bayesian analysis. This makes them invaluable for parameter estimation, model comparison, and uncertainty quantification in almost any astrophysical problem.

Another reason for the appeal of Bayesian methods is their contrast with frequentist approaches. Frequentist methods, which were dominant through much of the 20th century, emphasize point estimates and confidence intervals derived from repeated sampling arguments. Bayesian inference, in contrast, provides full posterior probability distributions for parameters, naturally incorporating prior information from physics or earlier observations. This framework is particularly powerful in astrophysics, where data are sparse, noisy, and often incomplete.

The systematic adoption of Bayesian statistics in astronomy gained momentum in the late 20th century. Cosmologists applied Bayesian methods to the cosmic microwave background; for example, works by \citet{tegmarkKarhunenLoeveEigenvalueProblems1997} and others showing how to extract cosmological parameters from noisy sky maps. In exoplanet science, Bayesian inference became standard in the 1990s and 2000s for modeling radial velocity curves and transit signals, which proved successful as datasets grew larger and more precise \citep{gregoryBayesianAnalysisExtrasolar2005}. Pulsar timing, supernova cosmology, and gravitational lens modeling similarly saw Bayesian methods supplanted heuristic or purely frequentist treatments. The trend reflects a broader recognition that many of astronomy’s hardest problems demand not just point estimates, but principled uncertainty quantification.

This review adopts a QA perspective and centers on Bayesian inference and MCMC because of their flexibility, principled uncertainty quantification, and growing ubiquity across astrophysics. In \hyperref[sec:Methodology]{Section II}, the methodology section develops a working foundation: it reviews Bayesian statistics, motivates priors and likelihood construction in realistic astronomical settings, and walks through toy examples before escalating to domain-relevant formulations; it then introduces Monte Carlo methods and builds to Markov Chain Monte Carlo, deriving the Metropolis family and related samplers, and provides step-by-step Python implementations on simple problems.

\hyperref[placeholder]{Section III} presents three focused case studies that illustrate how Bayesian–MCMC pipelines advance frontiers in different subfields while confronting distinct sources of noise, degeneracy, and computational load. For exoplanet direct detection, the section sketches the observational context and the central challenge of disentangling planetary signals from stellar activity and disk structure; it then outlines joint stellar–planet modeling with Gaussian processes and Bayesian classification frameworks for robust detections and non-detections, highlighting benefits and failure modes in low SNR regimes. For CMB parameter estimation, it explains geometric degeneracies in the power spectrum and shows how MCMC accelerators and parallelizable frameworks enable efficient exploration and evidence calculations, including tradeoffs between sampler sophistication and wall-clock efficiency. For gravitational-wave inference, it frames waveform fitting in high-dimensional parameter spaces, discusses the cost of likelihood evaluations, and motivates gradient-informed MCMC strategies that reduce computation without compromising accuracy, with notes on priors, calibration systematics, and real-time constraints.

Finally, \hyperref[placeholder]{Section IV} and \hyperref[placeholder]{V} compare cases where Bayesian–MCMC excels with those where complementary methods are more appropriate, identify methodological gaps revealed by the case studies (e.g., scalable likelihoods, robust priors, multimodal posterior handling), and outline opportunities for future work in astrophysics and related fields that face similar statistical and computational challenges.

\section{Methodology}
\label{sec:Methodology}

\subsection{Bayesian Statistics}
\subsubsection{Foundations}
% You can also talk about the difference between Bayesian and Frequentist Stats here
\subsubsection{Prior Construction for Astrophysics}
\subsubsection{Likelihood Construction}

\subsection{Monte Carlo Methods}
\subsubsection{High-Dimensional Integration}
\subsubsection{Basic Limitations}

\subsection{Markov Chain Monte Carlo Theory}
\subsubsection{Markov Chains}
\subsubsection{The Metropolis Algorithm}
\subsubsection{Advanced MCMC}

\subsection{Implementations}
\subsubsection{Software Ecosystem}
\subsubsection{Common Challenges in Astrophysics}



\bibliography{references}

\end{document}
